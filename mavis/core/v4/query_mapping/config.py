# # Contains all configs for all supported languages


from core.constants import STRING_CHAR
from core.v4.query_runner import big_query, databricks, mssql_odbc, pg, snowflake

ALL_TYPES = ["timestamp", "string", "number", "boolean"]
CASTING = [
    "date",
    "timestamp",
    "string",
    "text",
    "bigint",
    "integer",
    "float",
    "number",
    "boolean",
]

DATE_SECONDS = dict(
    year=60 * 60 * 24 * 465,
    quarter=60 * 60 * 24 * 91,
    month=60 * 60 * 24 * 30,
    week=60 * 60 * 24 * 7,
    day=60 * 60 * 24,
    hour=60 * 60,
    minute=60,
    second=1,
)
LOWER_OPERATORS = (
    "contains",
    "contains_any",
    "ends_with",
    "starts_with",
    "not_contains_any",
    "not_contains",
    "not_ends_with",
    "not_starts_with",
)

AGG_TO_WINDOW_FUNC = {
    ("count", "ever"): "count_window",
    ("count_distinct", "ever"): "count_window_distinct",
    ("sum", "ever"): "sum_window",
    ("max", "ever"): "max_window",
    ("min", "ever"): "min_window",
    ("average", "ever"): "average_window",
    # before
    ("count", "before"): "count_window_before",
    # ("count_distinct", "before"): "count_window_distinct_before",
    ("sum", "before"): "sum_window_before",
    ("max", "before"): "max_window_before",
    ("min", "before"): "min_window_before",
    ("average", "before"): "average_window_before",
    # After
    ("count", "after"): "count_window_after",
    # ("count_distinct", "after"): "count_window_distinct_after",
    ("sum", "after"): "sum_window_after",
    ("max", "after"): "max_window_after",
    ("min", "after"): "min_window_after",
    ("average", "after"): "average_window_after",
}

CONFIG = dict(
    redshift=dict(
        query_runner=pg.Redshift,
        week_offset=1,
        require_alias=False,
        reserved_words=[
            "aes128",
            "aes256",
            "all",
            "allowoverwrite",
            "analyse",
            "analyze",
            "and",
            "any",
            "array",
            "as",
            "asc",
            "authorization",
            "backup",
            "between",
            "binary",
            "blanksasnull",
            "both",
            "bytedict",
            "bzip2",
            "case",
            "cast",
            "check",
            "collate",
            "column",
            "constraint",
            "create",
            "credentials",
            "cross",
            "current_date",
            "current_time",
            "current_timestamp",
            "current_user",
            "current_user_id",
            "default",
            "deferrable",
            "deflate",
            "defrag",
            "delta",
            "delta32k",
            "desc",
            "disable",
            "distinct",
            "do",
            "else",
            "emptyasnull",
            "enable",
            "encode",
            "encrypt",
            "encryption",
            "end",
            "except",
            "explicit",
            "false",
            "for",
            "foreign",
            "freeze",
            "from",
            "full",
            "globaldict256",
            "globaldict64k",
            "grant",
            "group",
            "gzip",
            "having",
            "identity",
            "ignore",
            "ilike",
            "in",
            "initially",
            "inner",
            "intersect",
            "into",
            "is",
            "isnull",
            "join",
            "leading",
            "left",
            "like",
            "limit",
            "localtime",
            "localtimestamp",
            "lun",
            "luns",
            "lzo",
            "lzop",
            "minus",
            "mostly13",
            "mostly32",
            "mostly8",
            "natural",
            "new",
            "not",
            "notnull",
            "nulls",
            "off",
            "offline",
            "offset",
            "oid",
            "old",
            "on",
            "only",
            "open",
            "or",
            "order",
            "outer",
            "overlaps",
            "parallel",
            "partition",
            "percent",
            "permissions",
            "placing",
            "primary",
            "raw",
            "readratio",
            "recover",
            "references",
            "respect",
            "rejectlog",
            "resort",
            "restore",
            "right",
            "select",
            "session_user",
            "similar",
            "snapshot",
            "some",
            "sysdate",
            "system",
            "table",
            "tag",
            "tdes",
            "text255",
            "text32k",
            "then",
            "timestamp",
            "to",
            "top",
            "trailing",
            "true",
            "truncatecolumns",
            "union",
            "unique",
            "user",
            "using",
            "verbose",
            "wallet",
            "when",
            "where",
            "with",
            "without",
        ],
        value_quotes="'",
        column_quotes='"',
        cast_mapping=dict(
            date="DATE",
            timestamp="TIMESTAMP",
            string=f"VARCHAR({STRING_CHAR})",
            text="VARCHAR(65535)",
            bigint="BIGINT",
            integer="INTEGER",
            float="FLOAT",
            number="FLOAT",
            boolean="BOOL",
            json="SUPER",
            structure="CAST({definition} AS {cast})",
        ),
        json_structure='{column}."{key}"',
        create_table_addons=[
            "DISTSTYLE {diststyle}",
            "DISTKEY({distkey})",
            "SORTKEY({sortkey})",
            "SORTKEY {sortmode}",
        ],
        timezone_structure="convert_timezone('{timezone}', {definition})",
        # series_table="generate_series(1,{count},1)",
        operators=dict(
            contains="{left} ilike '%{right_value}%'",
            starts_with="{left} ilike '{right_value}%'",
            ends_with="{left} ilike '%{right_value}'",
            is_null="{left} is NULL",
            is_in="{left} in ({right}) ",
            greater_than="{left} > {right} ",
            less_than="{left} < {right} ",
            greater_than_equal="{left} >= {right} ",
            less_than_equal="{left} <= {right} ",
            equal="{left} = {right} ",
            is_empty="{left} = '' ",
            not_is_empty="{left} <> '' ",
            not_equal="{left} <> {right} ",
            not_contains="{left} not ilike '%{right_value}%'",
            not_starts_with="{left} not ilike '{right_value}%'",
            not_ends_with="{left} not ilike '%{right_value}'",
            not_is_null="{left} is not NULL",
            not_is_in="{left} not in ({right})",
        ),
        functions=dict(
            to_json="object({key_value_pair})",
            epoch_to_timestamp="timestamp 'epoch' + {column} * interval '1 second'",
            timestamp_to_epoch="extract('epoch' from {column})",
            now="SYSDATE",
            abs="ABS({column})",
            to_timezone="CONVERT_TIMEZONE({timezone}, {column})",
            date_trunc="DATE_TRUNC('{datepart}', {column})",
            date_add="DATE_ADD('{datepart}', {number}, {column})",
            date_diff="FLOOR(DATE_DIFF('second', {from_column}, {to_column})/{DATE_SECONDS})",
            date_diff_boundary="DATE_DIFF('{datepart}', {from_column}, {to_column})",
            date_to_now="FLOOR(DATE_DIFF('second', {from_column}, SYSDATE)/{DATE_SECONDS})",
            date_trunc_resolution="DATE_ADD('{datepart}', (floor(DATE_DIFF('{datepart}', '1990-01-01'::TIMESTAMP, {column})/{number})*{number})::INTEGER, '1990-01-01'::TIMESTAMP)",
            time_add="DATE_ADD('{datepart}', {number}, {column})",
            time_diff="FLOOR(DATE_DIFF('second', {from_column}, {to_column})/{DATE_SECONDS})",
            time_to_now="FLOOR(DATE_DIFF('second', {from_column}, SYSDATE)/{DATE_SECONDS})",
            time_trunc_resolution="DATE_ADD('{datepart}', (floor(DATE_DIFF('{datepart}', '1990-01-01'::TIMESTAMP, {column})/{number})*{number})::INTEGER, '1990-01-01'::TIMESTAMP)",
            date_part="DATE_PART('{datepart}', {column})",
            nullif="nullif({column}, {value_column})",
            exists="CASE WHEN {column} is not NULL THEN 1 ELSE 0 END",
            not_exists="CASE WHEN {column} is NULL THEN 1 ELSE 0 END",
            condition_flag="CASE WHEN {condition} THEN 1 ELSE 0 END",
            power="power({column}, {exponent_column})",
            nvl="NVL({first_column}, {second_column})",
            coalesce="COALESCE({first_column}, {second_column})",
            replace="REPLACE({column}, '{remove_string}', '{add_string}')",
            lower="LOWER({column})",
            length="LENGTH({column})",
            snake_case="LOWER(REPLACE({column}, ' ', '_'))",
            title="INITCAP(REPLACE({column}, '_', ' '))",
            concat="CONCAT({first_column}, {second_column})",
            substring="SUBSTRING({column}, {position}, {length})",
            strpos="STRPOS({column}, '{piece}')",
            random="random()",
            mod="mod({column}, {number})",
            decimate_number="FLOOR({column}/{number}) * {number}",
            floor="floor({column})",
            string_between="REPLACE(REPLACE(REGEXP_SUBSTR({column}, '{from_piece}([^{to_piece}]*){to_piece}'), '{from_piece}', ''),'{to_piece}', '')",
            regexp_extract="REGEXP_SUBSTR({column}, '{expression}')",
            regexp_count="REGEXP_COUNT({column},  '{expression}')",
            split_part="SPLIT_PART({column}, '{delimiter}', {index})",
            greatest="GREATEST({first_column}, {second_column})",
            least="LEAST({first_column}, {second_column})",
            sqrt="SQRT({column})",
            percentile_cont_window="PERCENTILE_CONT({percentile}) within group (ORDER BY {column}) over (partition by {group})",
            percentile_cont_all="PERCENTILE_CONT({percentile}) within group (ORDER BY {column}) over ()",
            ratio_to_report="RATIO_TO_REPORT({column}) OVER (PARTITION BY {group})",
            percent_of_total_all="RATIO_TO_REPORT({column}) OVER ()",
            row_number_empty="ROW_NUMBER() over ()",
            row_number_all="ROW_NUMBER() over (ORDER BY {order})",
            row_number_w_group="ROW_NUMBER() over (PARTITION by {group} ORDER BY {order})",
            string_agg="LISTAGG({column}) over (PARTITION by {group} ORDER BY {order})",
            count_window="COUNT({column}) over (PARTITION by {group})",
            count_window_distinct="COUNT( DISTINCT {column}) over (PARTITION by {group})",
            sum_window="SUM({column}) over (PARTITION by {group})",
            max_window="MAX({column}) over (PARTITION by {group})",
            min_window="MIN({column}) over (PARTITION by {group})",
            average_window="AVG(1.000*{column}) over (PARTITION by {group})",
            lead="LEAD({column}) over (PARTITION by {group} ORDER BY {order})",
            lag="LAG({column}) over (PARTITION by {group} ORDER BY {order})",
            lead_ignore_nulls="LEAD({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order})",
            lag_ignore_nulls="LAG({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order})",
            count_window_before="COUNT({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            count_window_distinct_before="COUNT( DISTINCT {column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            sum_window_before="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            max_window_before="MAX({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            min_window_before="MIN({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            average_window_before="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            count_window_after="COUNT({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            count_window_distinct_after="COUNT( DISTINCT {column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            sum_window_after="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            max_window_after="MAX({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            min_window_after="MIN({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            average_window_after="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lead_after="LEAD({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lag_after="LAG({column}) over (PARTITION by {group} ORDER BY {order} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lead_ignore_nulls_after="LEAD({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order}  ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lag_ignore_nulls_after="LAG({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order}  ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            count_window_distinct_all="COUNT(DISTINCT {column}) over ()",
            count_window_all="COUNT({column}) over ()",
            sum_window_all="SUM({column}) over ()",
            max_window_all="MAX({column}) over ()",
            min_window_all="MIN({column}) over ()",
            running_total="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            running_total_all="SUM({column}) over (ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            product_window="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (PARTITION BY {group}))",
            running_product_all="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (ORDER by {order} rows between unbounded preceding and current row))",
            running_product="EXP(SUM(LN(CAST(nullif({column}, 0) as FLOAT))) over (PARTITION by {group} ORDER by {order} rows between unbounded preceding and current row))",
            moving_average="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS {window_size} PRECEDING)",
            moving_average_all="AVG(1.000*{column}) over (ORDER BY {order} ROWS {window_size} PRECEDING)",
            dense_rank="DENSE_RANK() over (PARTITION by {group} ORDER BY {order})",
            dense_rank_all="DENSE_RANK() over ()",
            percent_rank="PERCENT_RANK() over (PARTITION by {group} ORDER BY {order})",
            lead_all="LEAD({column}) over (ORDER BY {order})",
            lag_all="LAG({column}) over (ORDER BY {order})",
            lead_offset="LEAD({column}, {offset}) over (PARTITION by {group} ORDER BY {order})",
            lag_offset="LAG({column}, {offset}) over (PARTITION by {group} ORDER BY {order})",
            lead_all_offset="LEAD({column}, {offset}) over (ORDER BY {order})",
            lag_all_offset="LAG({column}, {offset}) over (ORDER BY {order})",
            first_value_window="FIRST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            last_value_window="LAST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            first_value_window_before="FIRST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and current row)",
            last_value_window_after="LAST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between current row and unbounded following)",
            first_value_all="FIRST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            last_value_all="LAST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            json_extract="JSON_EXTRACT_PATH_TEXT({column}, '{key}')",
            window_func_w_group_and_order="{function}({column}) over (PARTITION by {group} ORDER BY {order})",
            window_func_w_group_and_order_w_preceeding="{function}({column}) over (PARTITION by {group} ORDER BY {order} ROWS {window} PRECEDING)",
            window_func_w_group="{function}({column}) over (PARTITION by {group})",
            window_func_w_order="{function}({column}) over (ORDER by {order})",
            single_column_function="{function}({column})",
            two_column_function="{function}({first_column}, {second_column})",
            string="{definition}",
        ),
        agg_functions=dict(
            count_all="COUNT(1)",
            count="COUNT({column})",
            count_distinct="COUNT(DISTINCT {column})",
            sum="SUM({column})",
            average="AVG(1.0000*{column})",
            max="MAX({column})",
            min="MIN({column})",
            stddev="STDDEV({column})",
            list_agg="LISTAGG( {column}, ', ') WITHIN GROUP (ORDER BY {base_column})",
            list_agg_unique="LISTAGG(DISTINCT {column}, ', ') WITHIN GROUP (ORDER BY {base_column})",
            median="PERCENTILE_CONT(.5) WITHIN GROUP (ORDER BY {column})",
            percentile_cont="PERCENTILE_CONT({percentile}) WITHIN GROUP (ORDER BY {column})",
            first_value="NULLIF(SUBSTRING(MIN(CONCAT(LEFT({base_column}, 19), NVL({column},''))),20, 1000),'')",
            last_value="NULLIF(SUBSTRING(MAX(CONCAT(LEFT({base_column}, 19), NVL({column},''))),20, 1000),'')",
            first_value_case="NULLIF(SUBSTRING(MIN(CASE WHEN {case} THEN CONCAT(LEFT({base_column}, 19), NVL({column},'')) END ),20, 1000),'')",
            last_value_case="NULLIF(SUBSTRING(MAX(CASE WHEN {case} THEN CONCAT(LEFT({base_column}, 19), NVL({column},'')) END ),20, 1000),'')",
        ),
    ),
    bigquery=dict(
        query_runner=big_query.BigQuery,
        week_offset=0,
        require_alias=False,
        reserved_words=[
            "all",
            "and",
            "any",
            "array",
            "as",
            "asc",
            "assert_rows_modified",
            "at",
            "between",
            "by",
            "case",
            "cast",
            "collate",
            "contains",
            "create",
            "cross",
            "cube",
            "current",
            "default",
            "define",
            "desc",
            "distinct",
            "else",
            "end",
            "enum",
            "escape",
            "except",
            "exclude",
            "exists",
            "extract",
            "false",
            "fetch",
            "following",
            "for",
            "from",
            "full",
            "group",
            "grouping",
            "groups",
            "hash",
            "having",
            "if",
            "ignore",
            "in",
            "inner",
            "intersect",
            "interval",
            "into",
            "is",
            "join",
            "lateral",
            "left",
            "like",
            "limit",
            "lookup",
            "merge",
            "natural",
            "new",
            "no",
            "not",
            "null",
            "nulls",
            "of",
            "on",
            "or",
            "order",
            "outer",
            "over",
            "partition",
            "preceding",
            "proto",
            "qualify",
            "range",
            "recursive",
            "respect",
            "right",
            "rollup",
            "rows",
            "select",
            "set",
            "some",
            "struct",
            "tablesample",
            "then",
            "to",
            "treat",
            "true",
            "unbounded",
            "union",
            "unnest",
            "using",
            "when",
            "where",
            "window",
            "with",
            "within",
        ],
        value_quotes='"',
        column_quotes="`",
        cast_mapping=dict(
            date="TIMESTAMP",
            timestamp="TIMESTAMP",
            string="STRING",
            text="STRING",
            bigint="INT64",
            integer="INT64",
            float="FLOAT64",
            number="FLOAT64",
            boolean="BOOL",
            json="JSON",
            structure="SAFE_CAST( {definition} AS {cast})",
        ),
        json_structure="JSON_VALUE({column}['{key}'])",
        timezone_structure='TIMESTAMP(DATETIME({definition},"{timezone}"))',
        series_table="UNNEST(GENERATE_ARRAY(1, {count}))",
        create_table_addons=[
            "PARTITION BY ({partition_column})",
            "CLUSTER BY {clusterkey}",
        ],
        operators=dict(
            contains='lower({left}) like "%{right_value}%"',
            starts_with='starts_with(lower({left}), "{right_value}")',
            ends_with='ends_with(lower({left}), "{right_value}")',
            is_null="{left} is NULL",
            is_in="{left} in ({right}) ",
            greater_than="{left} > {right} ",
            less_than="{left} < {right} ",
            greater_than_equal="{left} >= {right} ",
            less_than_equal="{left} <= {right} ",
            equal="{left} = {right} ",
            is_empty='{left} = "" ',
            not_is_empty='{left} <> "" ',
            not_equal="{left} <> {right} ",
            not_contains='lower({left}) not like "%{right_value}%"',
            not_starts_with='not starts_with(lower({left}), "{right_value}")',
            not_ends_with='not ends_with(lower({left}), "{right_value}")',
            not_is_null="{left} is not NULL",
            not_is_in="{left} not in ({right})",
        ),
        functions=dict(
            to_json="TO_JSON(STRUCT({value_as_key}))",
            epoch_to_timestamp="TIMESTAMP_SECONDS({column})",
            timestamp_to_epoch="UNIX_SECONDS({column})",
            now="CURRENT_TIMESTAMP()",
            abs="ABS({column})",
            to_timezone='TIMESTAMP(DATETIME({column}, "{timezone}"))',
            date_trunc="TIMESTAMP_TRUNC({column}, {datepart})",
            time_add="TIMESTAMP_ADD({column}, INTERVAL {number} {datepart})",
            time_diff="FLOOR(TIMESTAMP_DIFF({to_column}, {from_column}, second)/{DATE_SECONDS})",
            time_to_now="FLOOR(TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), {from_column}, second)/{DATE_SECONDS})",
            time_trunc_resolution="TIMESTAMP_ADD( TIMESTAMP('1990-01-01') , INTERVAL CAST(FLOOR(TIMESTAMP_DIFF({column},  TIMESTAMP('1990-01-01'), {datepart}) / {number}) * {number} as INT64) {datepart})",
            date_add="TIMESTAMP(DATE_ADD( DATE({column}), INTERVAL {number} {datepart}))",
            date_diff="FLOOR(TIMESTAMP_DIFF({to_column}, {from_column}, second)/{DATE_SECONDS})",
            date_diff_boundary="DATE_DIFF(DATE({to_column}), DATE({from_column}), {datepart})",
            date_to_now="FLOOR(TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), {from_column}, second)/{DATE_SECONDS})",
            date_trunc_resolution="DATE_ADD( DATE('1990-01-01') , INTERVAL CAST( FLOOR(DATE_DIFF(DATE({column}), DATE('1990-01-01'), {datepart}) / {number}) * {number} as INT64) {datepart} )",
            date_part="EXTRACT({datepart} from {column})",
            nullif="nullif({column}, {value_column})",
            exists="CASE WHEN {column} is not NULL THEN 1 ELSE 0 END",
            not_exists="CASE WHEN {column} is NULL THEN 1 ELSE 0 END",
            condition_flag="CASE WHEN {condition} THEN 1 ELSE 0 END",
            power="power({column}, {exponent_column})",
            nvl="COALESCE({first_column}, {second_column})",
            coalesce="COALESCE({first_column}, {second_column})",
            replace='REPLACE({column}, "{remove_string}", "{add_string}")',
            lower="LOWER({column})",
            length="LENGTH({column})",
            snake_case='LOWER(REPLACE({column}, " ", "_"))',
            title='INITCAP(REPLACE({column}, "_", " "))',
            concat="CONCAT({first_column}, {second_column})",
            substring="SUBSTR({column}, {position}, {length})",
            split_part="SPLIT({column}, '{delimiter}')[ordinal({index})]",
            strpos='STRPOS({column}, "{piece}")',
            floor="floor({column})",
            random="RAND()",
            mod="mod({column}, {number})",
            decimate_number="FLOOR({column}/{number}) * {number}",
            string_between='NULLIF(REGEXP_EXTRACT({column}, r"(?i){from_piece}(.*){to_piece}") ,"")',
            regexp_extract='REGEXP_EXTRACT({column}, r"{expression}")',
            regexp_count="array_length(regexp_extract_all({column},  r'{expression}'))",
            greatest="GREATEST({first_column}, {second_column})",
            least="LEAST({first_column}, {second_column})",
            sqrt="SQRT({column})",
            percentile_cont_window="PERCENTILE_CONT({column}, {percentile}) over (partition by {group})",
            percentile_cont_all="PERCENTILE_CONT({column}, {percentile}) over ()",
            ratio_to_report="({column} / NULLIF(SUM({column}) OVER (PARTITION BY {group}),0))",
            percent_of_total_all="({column} / NULLIF(SUM({column}) OVER (),0))",
            row_number_empty="ROW_NUMBER() over ()",
            row_number_all="ROW_NUMBER() over (ORDER BY {order})",
            row_number_w_group="ROW_NUMBER() over (PARTITION by {group} ORDER BY {order})",
            string_agg="STRING_AGG({column}) over (PARTITION by {group} ORDER BY {order})",
            count_window_before="COUNT({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            count_window_distinct_before="COUNT( DISTINCT {column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            sum_window_before="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            max_window_before="MAX({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            min_window_before="MIN({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            average_window_before="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            count_window_after="COUNT({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            count_window_distinct_after="COUNT( DISTINCT {column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            sum_window_after="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            max_window_after="MAX({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            min_window_after="MIN({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            average_window_after="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lead_after="LEAD({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lag_after="LAG({column}) over (PARTITION by {group} ORDER BY {order} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lead_ignore_nulls_after="LEAD({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order}  ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lag_ignore_nulls_after="LAG({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order}  ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            count_window="COUNT({column}) over (PARTITION by {group})",
            count_window_distinct="COUNT( DISTINCT {column}) over (PARTITION by {group})",
            sum_window="SUM({column}) over (PARTITION by {group})",
            max_window="MAX({column}) over (PARTITION by {group})",
            min_window="MIN({column}) over (PARTITION by {group})",
            count_window_all="COUNT({column}) over ()",
            count_window_distinct_all="COUNT(DISTINCT {column}) over ()",
            sum_window_all="SUM({column}) over ()",
            max_window_all="MAX({column}) over ()",
            min_window_all="MIN({column}) over ()",
            average_window="AVG(1.000*{column}) over (PARTITION by {group})",
            running_total="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            running_total_all="SUM({column}) over (ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            product_window="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT64))) over (PARTITION BY {group}))",
            running_product_all="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT64))) over (ORDER by {order} rows between unbounded preceding and current row))",
            running_product="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT64))) over (PARTITION by {group} ORDER by {order} rows between unbounded preceding and current row))",
            moving_average="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS {window_size} PRECEDING)",
            moving_average_all="AVG(1.000*{column}) over (ORDER BY {order} ROWS {window_size} PRECEDING)",
            lead="LEAD({column}) over (PARTITION by {group} ORDER BY {order})",
            lag="LAG({column}) over (PARTITION by {group} ORDER BY {order})",
            lead_ignore_nulls="FIRST_VALUE({column} IGNORE NULLS) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW AND UNBOUNDED Following)",
            lag_ignore_nulls="LAST_VALUE({column} IGNORE NULLS) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW) ",
            dense_rank="DENSE_RANK() over (PARTITION by {group} ORDER BY {order})",
            dense_rank_all="DENSE_RANK() over ()",
            percent_rank="PERCENT_RANK() over (PARTITION by {group} ORDER BY {order})",
            lead_all="LEAD({column}) over (ORDER BY {order})",
            lag_all="LAG({column}) over (ORDER BY {order})",
            lead_offset="LEAD({column}, {offset}) over (PARTITION by {group} ORDER BY {order})",
            lag_offset="LAG({column}, {offset}) over (PARTITION by {group} ORDER BY {order})",
            lead_all_offset="LEAD({column}, {offset}) over (ORDER BY {order})",
            lag_all_offset="LAG({column}, {offset}) over (ORDER BY {order})",
            first_value_window="FIRST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            first_value_window_before="FIRST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and current row)",
            last_value_window_after="LAST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between current row and unbounded following)",
            last_value_window="LAST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            first_value_all="FIRST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            last_value_all="LAST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            json_extract=""" TRIM( JSON_EXTRACT({column}, "$.{key}"), '"') """,
            window_func_w_group_and_order="{function}({column}) over (PARTITION by {group} ORDER BY {order})",
            window_func_w_group_and_order_w_preceeding="{function}({column}) over (PARTITION by {group} ORDER BY {order} ROWS {window} PRECEDING)",
            window_func_w_group="{function}({column}) over (PARTITION by {group})",
            window_func_w_order="{function}({column}) over (ORDER by {order})",
            single_column_function="{function}({column})",
            two_column_function="{function}({first_column}, {second_column})",
            string="{definition}",
        ),
        agg_functions=dict(
            count_all="COUNT(1)",
            count="COUNT({column})",
            count_distinct="COUNT(DISTINCT {column})",
            sum="SUM({column})",
            average="AVG(1.000*{column})",
            max="MAX({column})",
            min="MIN({column})",
            stddev="STDDEV({column})",
            list_agg="STRING_AGG( {column}, ', ' ORDER BY {base_column})",
            list_agg_unique="STRING_AGG(DISTINCT {column}, ', ' ORDER BY {column})",
            first_value="NULLIF(SUBSTR(MIN(CONCAT(CAST(TIMESTAMP_TRUNC({base_column}, SECOND) AS string), COALESCE({column},'') )), 23, 1000), '')",
            last_value="NULLIF(SUBSTR(MAX(CONCAT(CAST(TIMESTAMP_TRUNC({base_column}, SECOND) AS string), coalesce({column},''))), 23, 1000), '')",
            first_value_case="NULLIF(SUBSTR(MIN(CASE WHEN {case} THEN CONCAT(CAST(TIMESTAMP_TRUNC({base_column}, SECOND) AS string), COALESCE({column},'') ) END), 23, 1000), '')",
            last_value_case="NULLIF(SUBSTR(MAX(CASE WHEN {case} THEN CONCAT(CAST(TIMESTAMP_TRUNC({base_column}, SECOND) AS string), coalesce({column},'')) END), 23, 1000), '')",
        ),
    ),
    pg=dict(
        query_runner=pg.PostgreSQL,
        week_offset=1,
        require_alias=True,
        reserved_words=[
            "all",
            "analyse",
            "analyze",
            "and",
            "any",
            "array",
            "as",
            "asc",
            "asymmetric",
            "both",
            "case",
            "cast",
            "check",
            "collate",
            "column",
            "constraint",
            "create",
            "current_catalog",
            "current_date",
            "current_role",
            "current_time",
            "current_timestamp",
            "current_user",
            "default",
            "deferrable",
            "desc",
            "distinct",
            "do",
            "else",
            "end",
            "except",
            "false",
            "fetch",
            "for",
            "foreign",
            "from",
            "grant",
            "group",
            "having",
            "in",
            "initially",
            "intersect",
            "into",
            "lateral",
            "leading",
            "limit",
            "localtime",
            "localtimestamp",
            "not",
            "null",
            "offset",
            "on",
            "only",
            "or",
            "order",
            "placing",
            "primary",
            "references",
            "returning",
            "select",
            "session_user",
            "some",
            "symmetric",
            "table",
            "then",
            "to",
            "trailing",
            "true",
            "union",
            "unique",
            "user",
            "using",
            "variadic",
            "when",
            "where",
            "window",
            "with",
            "role",
        ],
        value_quotes="'",
        column_quotes='"',
        cast_mapping=dict(
            date="DATE",
            timestamp="TIMESTAMP",
            string=f"VARCHAR({STRING_CHAR})",
            text="VARCHAR(65535)",
            bigint="BIGINT",
            integer="INTEGER",
            float="FLOAT",
            number="FLOAT",
            boolean="BOOL",
            json="JSONB",
            structure="CAST({definition} AS {cast})",
        ),
        json_structure="{column} ->> '{key}'",
        series_table="generate_series(1, {count})",
        create_table_addons=[
            # these indexes run for all tables (activity, customer, enrichment)
            # some are only relevant for certain tables. This is controlled by whether or not the values
            # in {brackets} have been defined for that table in run_transformations
            # general index
            "; CREATE INDEX {index_name} ON {table} ({index})",
            # activity_stream specific indexes
            "; CREATE INDEX {index_name}_{occurrence_index}_null_idx ON {table}(activity) where activity_occurrence IS NULL",
            "; CREATE INDEX {index_name}_{occurrence_index}_1_idx ON {table}(activity) where activity_occurrence = 1",
            "; CREATE INDEX {index_name}_{occurrence_index}_idx ON {table}(activity, activity_occurrence) where activity_occurrence > 1",
            "; CREATE INDEX {index_name}_{repeated_at_index}_idx ON {table}(activity) where activity_repeated_at IS NULL",
            "; CREATE INDEX {index_name}_{anonymous_customer_index}_idx ON {table}(anonymous_customer_id)",
            "; CREATE INDEX {index_name_part} ON {table} ({index})",
            # activity_stream specific indexes
            "; CREATE INDEX {index_name_part}_{occurrence_index}_null_idx ON {table}(activity_occurrence) where activity_occurrence IS NULL",
            "; CREATE INDEX {index_name_part}_{occurrence_index}_1_idx ON {table}(activity_occurrence) where activity_occurrence = 1",
            "; CREATE INDEX {index_name_part}_{repeated_at_index}_idx ON {table}(activity_repeated_at) where activity_repeated_at IS NULL",
            "; CREATE INDEX {index_name}_{customer_idx}_idx ON {table} (customer) where customer IS NOT NULL",
        ],
        timezone_structure="timezone('{timezone}', {definition})",
        operators=dict(
            contains="{left} ilike '%{right_value}%'",
            starts_with="{left} ilike '{right_value}%'",
            ends_with="{left} ilike '%{right_value}'",
            is_null="{left} is NULL",
            is_in="{left} in ({right}) ",
            greater_than="{left} > {right} ",
            less_than="{left} < {right} ",
            greater_than_equal="{left} >= {right} ",
            less_than_equal="{left} <= {right} ",
            equal="{left} = {right} ",
            is_empty="{left} = '' ",
            not_is_empty="{left} <> '' ",
            not_equal="{left} <> {right} ",
            not_contains="{left} not ilike '%{right_value}%'",
            not_starts_with="{left} not ilike '{right_value}%'",
            not_ends_with="{left} not ilike '%{right_value}'",
            not_is_null="{left} is not NULL",
            not_is_in="{left} not in ({right})",
        ),
        functions=dict(
            to_json="json_build_object({key_value_pair})",
            epoch_to_timestamp="to_timestamp({column})",
            timestamp_to_epoch="extract('epoch' from {column})",
            now="NOW()",
            abs="ABS({column})",
            to_timezone="timezone({timezone}, {column})",
            date_trunc="DATE_TRUNC('{datepart}', {column})",
            date_add="({column} + {number} * INTERVAL '1 {datepart}')",
            date_diff="floor(EXTRACT(EPOCH FROM ({to_column} - {from_column})) / {DATE_SECONDS})",
            date_diff_boundary="DATE_PART('{datepart}', {to_column} - {from_column})",
            date_to_now="floor(EXTRACT(EPOCH FROM (NOW() - {from_column})) / {DATE_SECONDS})",
            date_trunc_resolution="DATE_ADD('{datepart}', (floor(DATE_DIFF('{datepart}', '1990-01-01'::TIMESTAMP, {column})/{number})*{number})::INTEGER, '1990-01-01'::TIMESTAMP)",
            time_add="({column} + {number} * INTERVAL '1 {datepart}')",
            time_diff="floor(EXTRACT(EPOCH FROM ({to_column} - {from_column})) / {DATE_SECONDS})",
            time_to_now="floor(EXTRACT(EPOCH FROM (NOW() - {from_column})) / {DATE_SECONDS})",
            time_trunc_resolution="DATE_ADD('{datepart}', (floor(DATE_DIFF('{datepart}', '1990-01-01'::TIMESTAMP, {column})/{number})*{number})::INTEGER, '1990-01-01'::TIMESTAMP)",
            date_part="DATE_PART('{datepart}', {column})",
            nullif="nullif({column}, {value_column})",
            exists="CASE WHEN {column} is not NULL THEN 1 ELSE 0 END",
            not_exists="CASE WHEN {column} is NULL THEN 1 ELSE 0 END",
            condition_flag="CASE WHEN {condition} THEN 1 ELSE 0 END",
            power="power({column}, {exponent_column})",
            nvl="COALESCE({first_column}, {second_column})",
            coalesce="COALESCE({first_column}, {second_column})",
            replace="REPLACE({column}, '{remove_string}', '{add_string}')",
            lower="LOWER({column})",
            length="LENGTH({column})",
            snake_case="LOWER(REPLACE({column}, ' ', '_'))",
            title="INITCAP(REPLACE({column}, '_', ' '))",
            concat="CONCAT({first_column}, {second_column})",
            substring="SUBSTRING({column}, {position}, {length})",
            strpos="STRPOS({column}, '{piece}')",
            floor="floor({column})",
            random="random()",
            mod="mod({column}, {number})",
            decimate_number="FLOOR({column}/{number}) * {number}",
            string_between="CASE WHEN strpos({column}, '{from_piece}') >0 and strpos({column}, '{to_piece}') > 0 and length({column})- strpos(REVERSE({column}), '{to_piece}')> strpos({column}, '{from_piece}') THEN SUBSTRING({column}, strpos({column}, '{from_piece}') + 1 , length({column})- strpos(REVERSE({column}), '{to_piece}') - strpos({column}, '{from_piece}') ) END",
            split_part="SPLIT_PART({column}, {delimiter}, {index})",
            greatest="GREATEST({first_column}, {second_column})",
            least="LEAST({first_column}, {second_column})",
            sqrt="SQRT({column})",
            percentile_cont_window="PERCENTILE_CONT({percentile}) within group (ORDER BY {column}) over (partition by {group})",
            percentile_cont_all="PERCENTILE_CONT({percentile}) within group (ORDER BY {column})",
            ratio_to_report="{column} / NULLIF(SUM({column}) OVER (PARTITION BY {group}) , 0)",
            percent_of_total_all="{column} / NULLIF(SUM({column}) OVER (), 0)",
            row_number_empty="ROW_NUMBER() over ()",
            row_number_all="ROW_NUMBER() over (ORDER BY {order})",
            row_number_w_group="ROW_NUMBER() over (PARTITION by {group} ORDER BY {order})",
            string_agg="STRING_AGG({column}) over (PARTITION by {group} ORDER BY {order})",
            count_window="COUNT({column}) over (PARTITION by {group})",
            count_window_distinct="COUNT( DISTINCT {column}) over (PARTITION by {group})",
            sum_window="SUM({column}) over (PARTITION by {group})",
            max_window="MAX({column}) over (PARTITION by {group})",
            min_window="MIN({column}) over (PARTITION by {group})",
            count_window_before="COUNT({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            count_window_distinct_before="COUNT( DISTINCT {column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            sum_window_before="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            max_window_before="MAX({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            min_window_before="MIN({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            average_window_before="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            count_window_after="COUNT({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            count_window_distinct_after="COUNT( DISTINCT {column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            sum_window_after="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            max_window_after="MAX({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            min_window_after="MIN({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            average_window_after="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lead_after="LEAD({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lag_after="LAG({column}) over (PARTITION by {group} ORDER BY {order} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lead_ignore_nulls_after="LEAD({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order}  ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lag_ignore_nulls_after="LAG({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order}  ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            count_window_all="COUNT({column}) over ()",
            count_window_distinct_all="COUNT(DISTINCT {column}) over ()",
            sum_window_all="SUM({column}) over ()",
            max_window_all="MAX({column}) over ()",
            min_window_all="MIN({column}) over ()",
            average_window="AVG(1.000*{column}) over (PARTITION by {group})",
            running_total="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            running_total_all="SUM({column}) over (ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            product_window="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (PARTITION BY {group}))",
            running_product_all="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (ORDER by {order} rows between unbounded preceding and current row))",
            running_product="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (PARTITION by {group} ORDER by {order} rows between unbounded preceding and current row))",
            moving_average="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS {window_size} PRECEDING)",
            lead="LEAD({column}) over (PARTITION by {group} ORDER BY {order})",
            lag="LAG({column}) over (PARTITION by {group} ORDER BY {order})",
            lead_ignore_nulls="LEAD({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order})",
            lag_ignore_nulls="LAG({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order})",
            lead_all="LEAD({column}) over (ORDER BY {order})",
            lag_all="LAG({column}) over (ORDER BY {order})",
            dense_rank="DENSE_RANK() over (PARTITION by {group} ORDER BY {order})",
            dense_rank_all="DENSE_RANK() over ()",
            percent_rank="PERCENT_RANK() over (PARTITION by {group} ORDER BY {order})",
            lead_offset="LEAD({column}, {offset}) over (PARTITION by {group} ORDER BY {order})",
            lag_offset="LAG({column}, {offset}) over (PARTITION by {group} ORDER BY {order})",
            lead_all_offset="LEAD({column}, {offset}) over (ORDER BY {order})",
            lag_all_offset="LAG({column}, {offset}) over (ORDER BY {order})",
            first_value_window="FIRST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            last_value_window="LAST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            first_value_window_before="FIRST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and current row)",
            last_value_window_after="LAST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between current row and unbounded following)",
            first_value_all="FIRST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            last_value_all="LAST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            window_func_w_group_and_order="{function}({column}) over (PARTITION by {group} ORDER BY {order})",
            window_func_w_group_and_order_w_preceeding="{function}({column}) over (PARTITION by {group} ORDER BY {order} ROWS {window} PRECEDING)",
            window_func_w_group="{function}({column}) over (PARTITION by {group})",
            window_func_w_order="{function}({column}) over (ORDER by {order})",
            json_extract="({column}::json->'{key}')",
            single_column_function="{function}({column})",
            two_column_function="{function}({first_column}, {second_column})",
            string="{definition}",
        ),
        agg_functions=dict(
            count_all="COUNT(1)",
            count="COUNT({column})",
            count_distinct="COUNT(DISTINCT {column})",
            sum="SUM({column})",
            average="AVG(1.000 * {column})",
            max="MAX({column})",
            min="MIN({column})",
            stddev="STDDEV({column})",
            list_agg="STRING_AGG( {column}, ', ' ORDER BY {base_column})",
            list_agg_unique="STRING_AGG( DISTINCT {column}, ', ' ORDER BY {base_column})",
            median="PERCENTILE_CONT(.5) WITHIN GROUP (ORDER BY {column})",
            percentile_cont="PERCENTILE_CONT({percentile}) WITHIN GROUP (ORDER BY {column})",
            first_value="NULLIF(SUBSTRING(MIN(CONCAT(DATE_TRUNC('second', {base_column}), COALESCE({column},''))),20, 1000),'')",
            last_value="NULLIF(SUBSTRING(MAX(CONCAT(DATE_TRUNC('second', {base_column}), COALESCE({column},''))),20, 1000),'')",
            first_value_case="NULLIF(SUBSTRING(MIN(CASE WHEN {case} THEN CONCAT(DATE_TRUNC('second', {base_column}), COALESCE({column},'')) END) ,20, 1000),'')",
            last_value_case="NULLIF(SUBSTRING(MAX(CASE WHEN {case} THEN CONCAT(DATE_TRUNC('second', {base_column}), COALESCE({column},'')) END) ,20, 1000),'')",
        ),
    ),
    snowflake=dict(
        query_runner=snowflake.Snowflake,
        week_offset=1,
        require_alias=False,
        reserved_words=[
            "account",
            "all",
            "alter",
            "and",
            "any",
            "as",
            "between",
            "by",
            "case",
            "cast",
            "check",
            "column",
            "connect",
            "connection",
            "constraint",
            "create",
            "cross",
            "current",
            "current_date",
            "current_time",
            "current_timestamp",
            "current_user",
            "database",
            "delete",
            "distinct",
            "drop",
            "else",
            "exists",
            "false",
            "following",
            "for",
            "from",
            "full",
            "grant",
            "group",
            "gscluster",
            "having",
            "ilike",
            "in",
            "increment",
            "inner",
            "insert",
            "intersect",
            "into",
            "is",
            "issue",
            "join",
            "lateral",
            "left",
            "like",
            "localtime",
            "localtimestamp",
            "minus",
            "natural",
            "not",
            "null",
            "of",
            "on",
            "or",
            "order",
            "organization",
            "qualify",
            "regexp",
            "revoke",
            "right",
            "rlike",
            "row",
            "rows",
            "sample",
            "schema",
            "select",
            "set",
            "some",
            "start",
            "table",
            "tablesample",
            "then",
            "to",
            "trigger",
            "true",
            "try_cast",
            "union",
            "unique",
            "update",
            "using",
            "values",
            "view",
            "when",
            "whenever",
            "where",
            "with",
            "role",
        ],
        value_quotes="'",
        column_quotes="",
        cast_mapping=dict(
            date="DATE",
            timestamp="TIMESTAMP",
            string=f"VARCHAR({STRING_CHAR})",
            text="TEXT",
            bigint="BIGINT",
            integer="INTEGER",
            float="FLOAT",
            number="FLOAT",
            boolean="BOOLEAN",
            json="OBJECT",
            structure="CAST({definition} AS {cast})",
        ),
        json_structure="{column}['{key}']",
        create_table_addons=["CLUSTER BY {cluster_column}"],
        timezone_structure="convert_timezone('UTC', '{timezone}', {definition})::timestamp_ntz",
        series_table="(SELECT seq4()+1 as num FROM table (generator(rowcount => {count})))",
        operators=dict(
            contains="{left} ilike '%{right_value}%'",
            starts_with="{left} ilike '{right_value}%'",
            ends_with="{left} ilike '%{right_value}'",
            is_null="{left} is NULL",
            is_in="{left} in ({right}) ",
            greater_than="{left} > {right} ",
            less_than="{left} < {right} ",
            greater_than_equal="{left} >= {right} ",
            less_than_equal="{left} <= {right} ",
            equal="{left} = {right} ",
            is_empty="{left} = '' ",
            not_is_empty="{left} <> '' ",
            not_equal="{left} <> {right} ",
            not_contains="{left} not ilike '%{right_value}%'",
            not_starts_with="{left} not ilike '{right_value}%'",
            not_ends_with="{left} not ilike '%{right_value}'",
            not_is_null="{left} is not NULL",
            not_is_in="{left} not in ({right})",
        ),
        functions=dict(
            to_json="object_construct({key_value_pair})",
            epoch_to_timestamp="to_timestamp({column})",
            timestamp_to_epoch="date_part(epoch_second, {column})",
            now="SYSDATE()",
            abs="ABS({column})",
            to_timezone="CONVERT_TIMEZONE('UTC', {timezone}, {column})",
            date_trunc="DATE_TRUNC({datepart}, {column})",
            date_add="DATEADD({datepart}, {number}, {column})",
            date_diff="FLOOR(DATEDIFF(second, {from_column}, {to_column})/{DATE_SECONDS})",
            date_diff_boundary="DATEDIFF({datepart}, {from_column}, {to_column})",
            date_to_now="FLOOR(DATEDIFF(second, {from_column},  SYSDATE())/{DATE_SECONDS})",
            date_trunc_resolution="DATEADD({datepart}, (floor(DATEDIFF({datepart}, '1990-01-01'::TIMESTAMP, {column})/{number})*{number})::INTEGER, '1990-01-01'::TIMESTAMP)",
            time_add="DATEADD({datepart}, {number}, {column})",
            time_diff="FLOOR(DATEDIFF(second, {from_column}, {to_column})/{DATE_SECONDS})",
            time_to_now="FLOOR(DATEDIFF(second, {from_column},  SYSDATE())/{DATE_SECONDS})",
            time_trunc_resolution="DATEADD({datepart}, (floor(DATEDIFF({datepart}, '1990-01-01'::TIMESTAMP, {column})/{number})*{number})::INTEGER, '1990-01-01'::TIMESTAMP)",
            date_part="DATE_PART({datepart}, {column})",
            nullif="NULLIF({column}, {value_column})",
            exists="CASE WHEN {column} is not NULL THEN 1 ELSE 0 END",
            not_exists="CASE WHEN {column} is NULL THEN 1 ELSE 0 END",
            condition_flag="CASE WHEN {condition} THEN 1 ELSE 0 END",
            power="power({column}, {exponent_column})",
            nvl="COALESCE({first_column}, {second_column})",
            coalesce="COALESCE({first_column}, {second_column})",
            replace="REPLACE({column}, '{remove_string}', '{add_string}')",
            lower="LOWER({column})",
            length="LENGTH({column})",
            snake_case="LOWER(REPLACE({column}, ' ', '_'))",
            title="INITCAP(REPLACE({column}, '_', ' '))",
            concat="CONCAT({first_column}, {second_column})",
            substring="SUBSTRING({column}, {position}, {length})",
            strpos="position('{piece}', {column})",
            random="uniform(0::float, 1::float, random())",
            mod="mod({column}, {number})",
            decimate_number="FLOOR({column}/{number}) * {number}",
            floor="floor({column})",
            string_between="NULLIF(CASE WHEN position({column}, '{from_piece}') >0 and position({column}, '{to_piece}') > 0 and length({column}) - position(REVERSE({column}), '{to_piece}')> position({column}, '{from_piece}') THEN SUBSTRING({column}, position({column}, '{from_piece}') + 1 , length({column})- position(REVERSE({column}), '{to_piece}') - position({column}, '{from_piece}') ) END, '')",
            split_part="SPLIT_PART({column}, {delimiter}, {index})",
            regexp_extract="REGEXP_SUBSTR({column}, '{expression}')",
            regexp_count="REGEXP_COUNT({column},  '{expression}')",
            greatest="GREATEST({first_column}, {second_column})",
            least="LEAST({first_column}, {second_column})",
            sqrt="SQRT({column})",
            percentile_cont_window="PERCENTILE_CONT({percentile}) within group (ORDER BY {column}) over (partition by {group})",
            percentile_cont_all="PERCENTILE_CONT({percentile}) within group (ORDER BY {column}) over ()",
            ratio_to_report="RATIO_TO_REPORT({column}) OVER (PARTITION BY {group})",
            percent_of_total_all="RATIO_TO_REPORT({column}) OVER ()",
            row_number_empty="ROW_NUMBER() over ()",
            row_number_all="ROW_NUMBER() over (ORDER BY {order})",
            row_number_w_group="ROW_NUMBER() over (PARTITION by {group} ORDER BY {order})",
            string_agg="LISTAGG({column}) over (PARTITION by {group} ORDER BY {order})",
            count_window="COUNT({column}) over (PARTITION by {group})",
            count_window_distinct="COUNT( DISTINCT {column}) over (PARTITION by {group})",
            sum_window="SUM({column}) over (PARTITION by {group})",
            max_window="MAX({column}) over (PARTITION by {group})",
            min_window="MIN({column}) over (PARTITION by {group})",
            count_window_before="COUNT({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            count_window_distinct_before="COUNT( DISTINCT {column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            sum_window_before="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            max_window_before="MAX({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            min_window_before="MIN({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            average_window_before="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            count_window_after="COUNT({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            count_window_distinct_after="COUNT( DISTINCT {column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            sum_window_after="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            max_window_after="MAX({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            min_window_after="MIN({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            average_window_after="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lead_after="LEAD({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lag_after="LAG({column}) over (PARTITION by {group} ORDER BY {order} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lead_ignore_nulls_after="LEAD({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order}  ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lag_ignore_nulls_after="LAG({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order}  ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            count_window_all="COUNT({column}) over ()",
            count_window_distinct_all="COUNT(DISTINCT {column}) over ()",
            sum_window_all="SUM({column}) over ()",
            max_window_all="MAX({column}) over ()",
            min_window_all="MIN({column}) over ()",
            average_window="AVG(1.000*{column}) over (PARTITION by {group})",
            running_total="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            running_total_all="SUM({column}) over (ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            product_window="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (PARTITION BY {group}))",
            running_product_all="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (ORDER by {order} rows between unbounded preceding and current row))",
            running_product="EXP(SUM(LN(CAST(nullif({column}, 0) as FLOAT))) over (PARTITION by {group} ORDER by {order} rows between unbounded preceding and current row))",
            moving_average="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS {window_size} PRECEDING)",
            moving_average_all="AVG(1.000*{column}) over (ORDER BY {order} ROWS {window_size} PRECEDING)",
            lead="LEAD({column}) over (PARTITION by {group} ORDER BY {order})",
            lag="LAG({column}) over (PARTITION by {group} ORDER BY {order})",
            lead_ignore_nulls="LEAD({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order})",
            lag_ignore_nulls="LAG({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order})",
            dense_rank="DENSE_RANK() over (PARTITION by {group} ORDER BY {order})",
            dense_rank_all="DENSE_RANK() over ()",
            percent_rank="PERCENT_RANK() over (PARTITION by {group} ORDER BY {order})",
            lead_all="LEAD({column}) over (ORDER BY {order})",
            lag_all="LAG({column}) over (ORDER BY {order})",
            lead_offset="LEAD({column}, {offset}) over (PARTITION by {group} ORDER BY {order})",
            lag_offset="LAG({column}, {offset}) over (PARTITION by {group} ORDER BY {order})",
            lead_all_offset="LEAD({column}, {offset}) over (ORDER BY {order})",
            lag_all_offset="LAG({column}, {offset}) over (ORDER BY {order})",
            first_value_window="FIRST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            last_value_window="LAST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            first_value_window_before="FIRST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and current row)",
            last_value_window_after="LAST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between current row and unbounded following)",
            first_value_all="FIRST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            last_value_all="LAST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            json_extract="{column}:{key}",
            window_func_w_group_and_order="{function}({column}) over (PARTITION by {group} ORDER BY {order})",
            window_func_w_group_and_order_w_preceeding="{function}({column}) over (PARTITION by {group} ORDER BY {order} ROWS {window} PRECEDING)",
            window_func_w_group="{function}({column}) over (PARTITION by {group})",
            window_func_w_order="{function}({column}) over (ORDER by {order})",
            single_column_function="{function}({column})",
            two_column_function="{function}({first_column}, {second_column})",
            string="{definition}",
        ),
        agg_functions=dict(
            count_all="COUNT(1)",
            count="COUNT({column})",
            count_distinct="COUNT(DISTINCT {column})",
            sum="SUM({column})",
            average="AVG(1.0000*{column})",
            max="MAX({column})",
            min="MIN({column})",
            stddev="STDDEV({column})",
            list_agg="LISTAGG({column}, ', ') WITHIN GROUP (ORDER BY ts)",
            list_agg_unique="LISTAGG(DISTINCT {column}, ', ')",
            median="PERCENTILE_CONT(.5) WITHIN GROUP (ORDER BY {column})",
            percentile_cont="PERCENTILE_CONT({percentile}) WITHIN GROUP (ORDER BY {column})",
            first_value="NULLIF(SUBSTRING(MIN(CONCAT(LEFT(CAST({base_column} as string), 19), NVL({column},''))),20, 1000),'')",
            last_value="NULLIF(SUBSTRING(MAX(CONCAT(LEFT(CAST({base_column} as string), 19), NVL({column},''))),20, 1000),'')",
            first_value_case="NULLIF(SUBSTRING(MIN(CASE WHEN {case} THEN CONCAT(LEFT(CAST({base_column} as string), 19), NVL({column},'')) END ),20, 1000),'')",
            last_value_case="NULLIF(SUBSTRING(MAX(CASE WHEN {case} THEN CONCAT(LEFT(CAST({base_column} as string), 19), NVL({column},'')) END ),20, 1000),'')",
        ),
    ),
    mssql_odbc=dict(
        query_runner=mssql_odbc.SQLServerODBC,
        week_offset=1,
        require_alias=True,
        reserved_words=[],
        value_quotes="'",
        column_quotes='"',
        cast_mapping=dict(
            date="DATE",
            timestamp="DATETIME",
            string=f"VARCHAR({STRING_CHAR})",
            text="TEXT",
            bigint="BIGINT",
            integer="INT",
            float="FLOAT",
            number="FLOAT",
            boolean="BIT",
            json="nvarchar(max)",
            structure="CAST({definition} AS {cast})",
        ),
        json_structure="JSON_VALUE({column}, '$.{key}')",
        create_table_addons=[
            "; CREATE CLUSTERED COLUMNSTORE INDEX {index_name} ON {table} ORDER ({order_key})",
            "; CREATE INDEX {index_name}_idx ON {table} ({index_key})",
            "; CREATE INDEX {index_name}_{occurrence_index} ON {table} (activity_occurrence) where activity_occurrence = 1",
            "; CREATE INDEX {index_name}_{repeated_at_index}_idx ON {table} (activity_repeated_at) where activity_repeated_at IS NULL",
            "; CREATE INDEX {index_name}_{customer_idx}_idx ON {table} (customer) where customer IS NOT NULL",
        ],
        timezone_structure="CAST((({definition} AT TIME ZONE 'UTC') AT TIME ZONE '{timezone}') AS DATETIME)",
        operators=dict(
            contains="LOWER({left}) like '%{right_value}%'",
            starts_with="LOWER({left}) like '{right_value}%'",
            ends_with="LOWER({left}) like '%{right_value}'",
            is_null="{left} is NULL",
            is_in="{left} in ({right}) ",
            greater_than="{left} > {right} ",
            less_than="{left} < {right} ",
            greater_than_equal="{left} >= {right} ",
            less_than_equal="{left} <= {right} ",
            equal="{left} = {right} ",
            is_empty="{left} = '' ",
            not_is_empty="{left} <> '' ",
            not_equal="{left} <> {right} ",
            not_contains="LOWER({left}) not like '%{right_value}%'",
            not_starts_with="LOWER({left}) not like '{right_value}%'",
            not_ends_with="LOWER({left}) not like '%{right_value}'",
            not_is_null="{left} is not NULL",
            not_is_in="{left} not in ({right})",
        ),
        functions=dict(
            to_json="(SELECT {value_as_key} FOR JSON PATH, WITHOUT_ARRAY_WRAPPER)",
            epoch_to_timestamp="DATEADD(ss, {column}, '19700101')",
            timestamp_to_epoch="extract(epoch from {column})",
            now="CURRENT_TIMESTAMP",
            abs="ABS({column})",
            to_timezone="{column} AT TIME ZONE {timezone}",
            date_trunc="DATEADD({datepart}, datediff({datepart}, 0, {column}), 0)",
            date_add="DATEADD({datepart}, {number}, {column})",
            date_diff="FLOOR(DATEDIFF(second, {from_column}, {to_column})/{DATE_SECONDS})",
            date_diff_boundary="DATEDIFF({datepart}, {from_column}, {to_column})",
            date_to_now="FLOOR(DATEDIFF(second, {from_column}, CURRENT_TIMESTAMP)/{DATE_SECONDS})",
            date_trunc_resolution="DATE_ADD({datepart}, (FLOOR(DATE_DIFF({datepart}, '1990-01-01', {column})/{number})*{number})::INTEGER, '1990-01-01')",
            time_add="DATEADD({datepart}, {number}, {column})",
            time_diff="FLOOR(DATEDIFF(second, {from_column}, {to_column})/{DATE_SECONDS})",
            time_to_now="FLOOR(DATEDIFF(second, {from_column}, CURRENT_TIMESTAMP)/{DATE_SECONDS})",
            time_trunc_resolution="DATE_ADD({datepart}, (FLOOR(DATE_DIFF({datepart}, '1990-01-01', {column})/{number})*{number})::INTEGER, '1990-01-01')",
            date_part="DATEPART({datepart}, {column})",
            nullif="nullif({column}, {value_column})",
            exists="CASE WHEN {column} is not NULL THEN 1 ELSE 0 END",
            not_exists="CASE WHEN {column} is NULL THEN 1 ELSE 0 END",
            condition_flag="CASE WHEN {condition} THEN 1 ELSE 0 END",
            power="power({column}, {exponent_column})",
            nvl="COALESCE({first_column}, {second_column})",
            coalesce="COALESCE({first_column}, {second_column})",
            replace="REPLACE({column}, '{remove_string}', '{add_string}')",
            lower="LOWER({column})",
            length="LEN({column})",
            snake_case="LOWER(REPLACE({column}, ' ', '_'))",
            title="INITCAP(REPLACE({column}, '_', ' '))",
            concat="CONCAT({first_column}, {second_column})",
            substring="SUBSTRING({column}, {position}, {length})",
            strpos="CHARINDEX('{piece}', {column})",
            floor="FLOOR({column})",
            random="RAND(convert(varbinary, newid()))",
            mod="mod({column}, {number})",
            decimate_number="FLOOR({column}/{number}) * {number}",
            string_between="RTRIM(LTRIM(REGEXP_EXTRACT({column}, '{from_piece}([^{to_piece}]*){to_piece}'), '{from_piece}'),'{to_piece}')",
            split_part="SPLIT_PART({column}, {delimiter}, {index})",
            greatest="GREATEST({first_column}, {second_column})",
            least="LEAST({first_column}, {second_column})",
            sqrt="SQRT({column})",
            percentile_cont_window="PERCENTILE_CONT({percentile}) within group (ORDER BY {column}) over (partition by {group})",
            percentile_cont_all="PERCENTILE_CONT({percentile}) within group (ORDER BY {column}) over (PARTITION by (SELECT 100))",
            ratio_to_report="{column} / NULLIF(SUM({column}) OVER (PARTITION BY {group}), 0)",
            percent_of_total_all="{column} / NULLIF(SUM({column}) OVER (), 0)",
            row_number_empty="ROW_NUMBER() over (ORDER BY (SELECT 100))",
            row_number_all="ROW_NUMBER() over (ORDER BY {order})",
            row_number_w_group="ROW_NUMBER() over (PARTITION by {group} ORDER BY {order})",
            string_agg="STRING_AGG({column}) over (PARTITION by {group} ORDER BY {order})",
            count_window="COUNT({column}) over (PARTITION by {group})",
            count_window_distinct="COUNT( DISTINCT {column}) over (PARTITION by {group})",
            sum_window="SUM({column}) over (PARTITION by {group})",
            max_window="MAX({column}) over (PARTITION by {group})",
            min_window="MIN({column}) over (PARTITION by {group})",
            count_window_all="COUNT({column}) over ()",
            count_window_before="COUNT({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            count_window_distinct_before="COUNT( DISTINCT {column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            sum_window_before="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            max_window_before="MAX({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            min_window_before="MIN({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            average_window_before="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            count_window_after="COUNT({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            count_window_distinct_after="COUNT( DISTINCT {column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            sum_window_after="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            max_window_after="MAX({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            min_window_after="MIN({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            average_window_after="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lead_after="LEAD({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lag_after="LAG({column}) over (PARTITION by {group} ORDER BY {order} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lead_ignore_nulls_after="LEAD({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order}  ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lag_ignore_nulls_after="LAG({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order}  ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            count_window_distinct_all="COUNT(DISTINCT {column}) over ()",
            sum_window_all="SUM({column}) over ()",
            max_window_all="MAX({column}) over ()",
            min_window_all="MIN({column}) over ()",
            average_window="AVG(1.000*{column}) over (PARTITION by {group})",
            running_total="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            running_total_all="SUM({column}) over (ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            product_window="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (PARTITION BY {group}))",
            running_product_all="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (ORDER by {order} rows between unbounded preceding and current row))",
            running_product="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (PARTITION by {group} ORDER by {order} rows between unbounded preceding and current row))",
            moving_average="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS {window_size} PRECEDING)",
            lead="LEAD({column}) over (PARTITION by {group} ORDER BY {order})",
            lag="LAG({column}) over (PARTITION by {group} ORDER BY {order})",
            lead_ignore_nulls="LEAD({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order})",
            lag_ignore_nulls="LAG({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order})",
            dense_rank="DENSE_RANK() over (PARTITION by {group} ORDER BY {order})",
            dense_rank_all="DENSE_RANK() over ()",
            percent_rank="PERCENT_RANK() over (PARTITION by {group} ORDER BY {order})",
            lead_all="LEAD({column}) over (ORDER BY {order})",
            lag_all="LAG({column}) over (ORDER BY {order})",
            lead_offset="LEAD({column}, {offset}) over (PARTITION by {group} ORDER BY {order})",
            lag_offset="LAG({column}, {offset}) over (PARTITION by {group} ORDER BY {order})",
            lead_all_offset="LEAD({column}, {offset}) over (ORDER BY {order})",
            lag_all_offset="LAG({column}, {offset}) over (ORDER BY {order})",
            first_value_window="FIRST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            last_value_window="LAST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            first_value_window_before="FIRST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and current row)",
            last_value_window_after="LAST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between current row and unbounded following)",
            first_value_all="FIRST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            last_value_all="LAST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            window_func_w_group_and_order="{function}({column}) over (PARTITION by {group} ORDER BY {order})",
            window_func_w_group_and_order_w_preceeding="{function}({column}) over (PARTITION by {group} ORDER BY {order} ROWS {window} PRECEDING)",
            window_func_w_group="{function}({column}) over (PARTITION by {group})",
            window_func_w_order="{function}({column}) over (ORDER by {order})",
            json_extract="JSON_VALUE({column}, '$.{key}')",
            single_column_function="{function}({column})",
            two_column_function="{function}({first_column}, {second_column})",
            string="{definition}",
        ),
        agg_functions=dict(
            count_all="COUNT(1)",
            count="COUNT({column})",
            count_distinct="COUNT(DISTINCT {column})",
            sum="SUM({column})",
            average="AVG(1.000 * {column})",
            max="MAX({column})",
            min="MIN({column})",
            stddev="STDEV({column})",
            list_agg="STRING_AGG( {column}, ', ' ORDER BY {base_column})",
            median="PERCENTILE_CONT(.5) WITHIN GROUP (ORDER BY {column}) over (PARTITION by (SELECT 100))",
            percentile_cont="PERCENTILE_CONT({percentile}) WITHIN GROUP (ORDER BY {column}) over (PARTITION by (SELECT 100))",
            first_value="NULLIF(SUBSTRING(MIN(CONCAT(LEFT({base_column}, 19), COALESCE({column},''))),20, 1000),'')",
            last_value="NULLIF(SUBSTRING(MAX(CONCAT(LEFT({base_column}, 19), COALESCE({column},''))),20, 1000),'')",
            first_value_case="NULLIF(SUBSTRING(MIN(CASE WHEN {case} THEN CONCAT(LEFT({base_column}, 19), COALESCE({column},'')) END ),20, 1000),'')",
            last_value_case="NULLIF(SUBSTRING(MAX(CASE WHEN {case} THEN CONCAT(LEFT({base_column}, 19), COALESCE({column},'')) END ),20, 1000),'')",
        ),
    ),
    databricks=dict(
        # THIS IS HIVE
        query_runner=databricks.Databricks,
        week_offset=1,
        require_alias=False,
        reserved_words=[],
        value_quotes="'",
        column_quotes="",
        cast_mapping=dict(
            date="DATE",
            timestamp="TIMESTAMP",
            string="STRING",
            text="TEXT",
            bigint="BIGINT",
            integer="INTEGER",
            float="FLOAT",
            number="FLOAT",
            boolean="BOOL",
            json="STRING",
            structure="{cast}({definition})",
        ),
        json_structure="{column}:{key}",
        create_table_addons=[
            "PARTITIONED BY ({partition_by})",
        ],
        timezone_structure="from_utc_timestamp({definition}, '{timezone}')",
        # series_table="generate_series(1,{count},1)",
        series_table="range(1, {count})",
        operators=dict(
            contains="{left} ilike '%{right_value}%'",
            starts_with="{left} ilike '{right_value}%'",
            ends_with="{left} ilike '%{right_value}'",
            is_null="{left} is NULL",
            is_in="{left} in ({right}) ",
            greater_than="{left} > {right} ",
            less_than="{left} < {right} ",
            greater_than_equal="{left} >= {right} ",
            less_than_equal="{left} <= {right} ",
            equal="{left} = {right} ",
            is_empty="{left} = '' ",
            not_is_empty="{left} <> '' ",
            not_equal="{left} <> {right} ",
            not_contains="{left} not ilike '%{right_value}%'",
            not_starts_with="{left} not ilike '{right_value}%'",
            not_ends_with="{left} not ilike '%{right_value}'",
            not_is_null="{left} is not NULL",
            not_is_in="{left} not in ({right})",
        ),
        functions=dict(
            to_json="to_json(named_struct({key_value_pair}))",
            epoch_to_timestamp="timestamp 'epoch' + {column} * interval '1 second'",
            timestamp_to_epoch="extract('epoch' from {column})",
            now="NOW()",
            abs="ABS({column})",
            to_timezone="CONVERT_TIMEZONE({timezone}, {column})",
            date_trunc="DATE_TRUNC('{datepart}', {column})",
            date_add="dateadd({datepart}, {number}, {column})",
            date_diff="FLOOR(datediff(second, {from_column}, {to_column})/{DATE_SECONDS})",
            date_diff_boundary="datediff({datepart}, {from_column}, {to_column})",
            date_to_now="FLOOR(datediff(second, {from_column}, NOW())/{DATE_SECONDS})",
            date_trunc_resolution="dateadd({datepart}, (floor(datediff({datepart}, '1990-01-01'::TIMESTAMP, {column})/{number})*{number})::INTEGER, '1990-01-01'::TIMESTAMP)",
            time_add="dateadd({datepart}, {number}, {column})",
            time_diff="FLOOR(datediff(second, {from_column}, {to_column})/{DATE_SECONDS})",
            time_to_now="FLOOR(datediff(second, {from_column}, NOW())/{DATE_SECONDS})",
            time_trunc_resolution="dateadd({datepart}, (floor(datediff({datepart}, '1990-01-01'::TIMESTAMP, {column})/{number})*{number})::INTEGER, '1990-01-01'::TIMESTAMP)",
            date_part="DATE_PART('{datepart}', {column})",
            nullif="nullif({column}, {value_column})",
            exists="CASE WHEN {column} is not NULL THEN 1 ELSE 0 END",
            not_exists="CASE WHEN {column} is NULL THEN 1 ELSE 0 END",
            condition_flag="CASE WHEN {condition} THEN 1 ELSE 0 END",
            power="power({column}, {exponent_column})",
            nvl="NVL({first_column}, {second_column})",
            coalesce="COALESCE({first_column}, {second_column})",
            replace="REPLACE({column}, '{remove_string}', '{add_string}')",
            lower="LOWER({column})",
            length="LENGTH({column})",
            snake_case="LOWER(REPLACE({column}, ' ', '_'))",
            title="INITCAP(REPLACE({column}, '_', ' '))",
            concat="CONCAT({first_column}, {second_column})",
            substring="SUBSTRING({column}, {position}, {length})",
            strpos="STRPOS({column}, '{piece}')",
            random="random()",
            mod="mod({column}, {number})",
            decimate_number="FLOOR({column}/{number}) * {number}",
            floor="floor({column})",
            string_between="REPLACE(REPLACE(REGEXP_SUBSTR({column}, '{from_piece}([^{to_piece}]*){to_piece}'), '{from_piece}', ''),'{to_piece}', '')",
            regexp_extract="REGEXP_SUBSTR({column}, '{expression}')",
            regexp_count="REGEXP_COUNT({column},  '{expression}')",
            split_part="SPLIT_PART({column}, '{delimiter}', {index})",
            greatest="GREATEST({first_column}, {second_column})",
            least="LEAST({first_column}, {second_column})",
            sqrt="SQRT({column})",
            percentile_cont_window="PERCENTILE_CONT({percentile}) within group (ORDER BY {column}) over (partition by {group})",
            percentile_cont_all="PERCENTILE_CONT({percentile}) within group (ORDER BY {column}) over ()",
            ratio_to_report="{column} / NULLIF(SUM({column}) OVER (PARTITION BY {group}) , 0)",
            percent_of_total_all="{column} / NULLIF(SUM({column}) OVER () , 0)",
            row_number_empty="ROW_NUMBER() over ()",
            row_number_all="ROW_NUMBER() over (ORDER BY {order})",
            row_number_w_group="ROW_NUMBER() over (PARTITION by {group} ORDER BY {order})",
            string_agg="LISTAGG({column}) over (PARTITION by {group} ORDER BY {order})",
            count_window="COUNT({column}) over (PARTITION by {group})",
            count_window_distinct="COUNT( DISTINCT {column}) over (PARTITION by {group})",
            sum_window="SUM({column}) over (PARTITION by {group})",
            max_window="MAX({column}) over (PARTITION by {group})",
            min_window="MIN({column}) over (PARTITION by {group})",
            count_window_all="COUNT({column}) over ()",
            count_window_before="COUNT({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            count_window_distinct_before="COUNT( DISTINCT {column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            sum_window_before="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            max_window_before="MAX({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            min_window_before="MIN({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            average_window_before="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN UNBOUNDED PRECEDING and CURRENT ROW)",
            count_window_after="COUNT({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            count_window_distinct_after="COUNT( DISTINCT {column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            sum_window_after="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            max_window_after="MAX({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            min_window_after="MIN({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            average_window_after="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lead_after="LEAD({column}) over (PARTITION by {group} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lag_after="LAG({column}) over (PARTITION by {group} ORDER BY {order} ORDER BY {order} ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lead_ignore_nulls_after="LEAD({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order}  ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            lag_ignore_nulls_after="LAG({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order}  ROWS BETWEEN CURRENT ROW and UNBOUNDED FOLLOWING)",
            count_window_distinct_all="COUNT(DISTINCT {column}) over ()",
            sum_window_all="SUM({column}) over ()",
            max_window_all="MAX({column}) over ()",
            min_window_all="MIN({column}) over ()",
            average_window="AVG(1.000*{column}) over (PARTITION by {group})",
            running_total="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            running_total_all="SUM({column}) over (ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            product_window="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (PARTITION BY {group}))",
            running_product_all="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (ORDER by {order} rows between unbounded preceding and current row))",
            running_product="EXP(SUM(LN(CAST(nullif({column}, 0) as FLOAT))) over (PARTITION by {group} ORDER by {order} rows between unbounded preceding and current row))",
            moving_average="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS {window_size} PRECEDING)",
            moving_average_all="AVG(1.000*{column}) over (ORDER BY {order} ROWS {window_size} PRECEDING)",
            lead="LEAD({column}) over (PARTITION by {group} ORDER BY {order})",
            lag="LAG({column}) over (PARTITION by {group} ORDER BY {order})",
            lead_ignore_nulls="LEAD({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order})",
            lag_ignore_nulls="LAG({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order})",
            dense_rank="DENSE_RANK() over (PARTITION by {group} ORDER BY {order})",
            dense_rank_all="DENSE_RANK() over ()",
            percent_rank="PERCENT_RANK() over (PARTITION by {group} ORDER BY {order})",
            lead_all="LEAD({column}) over (ORDER BY {order})",
            lag_all="LAG({column}) over (ORDER BY {order})",
            lead_offset="LEAD({column}, {offset}) over (PARTITION by {group} ORDER BY {order})",
            lag_offset="LAG({column}, {offset}) over (PARTITION by {group} ORDER BY {order})",
            lead_all_offset="LEAD({column}, {offset}) over (ORDER BY {order})",
            lag_all_offset="LAG({column}, {offset}) over (ORDER BY {order})",
            first_value_window="FIRST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            last_value_window="LAST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            first_value_window_before="FIRST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and current row)",
            last_value_window_after="LAST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between current row and unbounded following)",
            first_value_all="FIRST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            last_value_all="LAST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            json_extract="JSON_EXTRACT_PATH_TEXT({column}, '{key}')",
            window_func_w_group_and_order="{function}({column}) over (PARTITION by {group} ORDER BY {order})",
            window_func_w_group_and_order_w_preceeding="{function}({column}) over (PARTITION by {group} ORDER BY {order} ROWS {window} PRECEDING)",
            window_func_w_group="{function}({column}) over (PARTITION by {group})",
            window_func_w_order="{function}({column}) over (ORDER by {order})",
            single_column_function="{function}({column})",
            two_column_function="{function}({first_column}, {second_column})",
            string="{definition}",
        ),
        agg_functions=dict(
            count_all="COUNT(1)",
            count="COUNT({column})",
            count_distinct="COUNT(DISTINCT {column})",
            sum="SUM({column})",
            average="AVG(1.0000*{column})",
            max="MAX({column})",
            min="MIN({column})",
            stddev="STDDEV({column})",
            list_agg="LISTAGG( {column}, ', ') WITHIN GROUP (ORDER BY  {base_column})",
            list_agg_unique="LISTAGG(DISTINCT {column}, ', ') WITHIN GROUP (ORDER BY  {base_column})",
            median="PERCENTILE_CONT(.5) WITHIN GROUP (ORDER BY {column})",
            percentile_cont="PERCENTILE_CONT({percentile}) WITHIN GROUP (ORDER BY {column})",
            first_value="NULLIF(SUBSTRING(MIN(CONCAT(LEFT({base_column}, 19), NVL({column},''))),20, 1000),'')",
            last_value="NULLIF(SUBSTRING(MAX(CONCAT(LEFT({base_column}, 19), NVL({column},''))),20, 1000),'')",
            first_value_case="NULLIF(SUBSTRING(MIN(CASE WHEN {case} THEN CONCAT(LEFT({base_column}, 19), NVL({column},'')) END ),20, 1000),'')",
            last_value_case="NULLIF(SUBSTRING(MAX(CASE WHEN {case} THEN CONCAT(LEFT({base_column}, 19), NVL({column},'')) END ),20, 1000),'')",
        ),
    ),
    # TODO: THESE ARE JUST A COPY
    mysql=dict(
        week_offset=1,
        require_alias=True,
        reserved_words=[],
        value_quotes="'",
        column_quotes='"',
        cast_mapping=dict(
            date="DATE",
            timestamp="DATETIME",
            string=f"nchar({STRING_CHAR})",
            text="TEXT",
            bigint="BIGINT",
            integer="INT",
            float="FLOAT",
            number="FLOAT",
            boolean="BIT",
            structure="CAST({definition} AS {cast})",
        ),
        create_table_addons=[
            # "; CREATE CLUSTERED COLUMNSTORE INDEX {index_name} ON {table} ({sortkey});"
        ],
        timezone_structure="({definition}) AT TIME ZONE '{timezone}'",
        operators=dict(
            contains="LOWER({left}) like '%{right_value}%'",
            starts_with="LOWER({left}) like '{right_value}%'",
            ends_with="LOWER({left}) like '%{right_value}'",
            is_null="{left} is NULL",
            is_in="{left} in ({right}) ",
            greater_than="{left} > {right} ",
            less_than="{left} < {right} ",
            greater_than_equal="{left} >= {right} ",
            less_than_equal="{left} <= {right} ",
            equal="{left} = {right} ",
            is_empty="{left} = '' ",
            not_is_empty="{left} <> '' ",
            not_equal="{left} <> {right} ",
            not_contains="LOWER({left}) not like '%{right_value}%'",
            not_starts_with="LOWER({left}) not like '{right_value}%'",
            not_ends_with="LOWER({left}) not like '%{right_value}'",
            not_is_null="{left} is not NULL",
            not_is_in="{left} not in ({right})",
        ),
        functions=dict(
            epoch_to_timestamp="FROM_UNIXTIME({column})",
            timestamp_to_epoch="TIMESTAMPDIFF(second, FROM_UNIXTIME(0), {column})",
            now="now()",
            abs="ABS({column})",
            to_timezone="{column} AT TIME ZONE {timezone}",
            date_trunc=" date_add('1900-01-01', interval TIMESTAMPDIFF({datepart}, '1900-01-01', {column}) {datepart})",
            date_add="DATE_ADD({column}, INTERVAL {number} {datepart})",
            date_diff="FLOOR(TIMESTAMPDIFF(second,{from_column}, {to_column})/{DATE_SECONDS})",
            date_diff_boundary="TIMESTAMPDIFF({datepart},{from_column}, {to_column})",
            date_to_now="FLOOR(TIMESTAMPDIFF(second, {from_column}, now())/{DATE_SECONDS})",
            date_trunc_resolution="DATE_ADD('1990-01-01', interval (FLOOR(TIMESTAMPDIFF({datepart}, '1990-01-01', {column})/{number})*{number}) {datepart})",
            time_add="DATE_ADD({column}, INTERVAL {number} {datepart})",
            time_diff="FLOOR(TIMESTAMPDIFF(second, {from_column}, {to_column})/{DATE_SECONDS})",
            time_to_now="FLOOR(TIMESTAMPDIFF(second, {from_column}, now())/{DATE_SECONDS})",
            time_trunc_resolution="DATE_ADD('1990-01-01', interval (FLOOR(TIMESTAMPDIFF({datepart}, '1990-01-01', {column})/{number})*{number}) {datepart})",
            date_part="extract({datepart} from {column})",
            nullif="nullif({column}, {value_column})",
            exists="CASE WHEN {column} is not NULL THEN 1 ELSE 0 END",
            not_exists="CASE WHEN {column} is NULL THEN 1 ELSE 0 END",
            condition_flag="CASE WHEN {condition} THEN 1 ELSE 0 END",
            power="power({column}, {exponent_column})",
            nvl="COALESCE({first_column}, {second_column})",
            coalesce="COALESCE({first_column}, {second_column})",
            replace="REPLACE({column}, '{remove_string}', '{add_string}')",
            lower="LOWER({column})",
            length="length({column})",
            snake_case="LOWER(REPLACE({column}, ' ', '_'))",
            title="INITCAP(REPLACE({column}, '_', ' '))",
            concat="CONCAT({first_column}, {second_column})",
            substring="SUBSTRING({column}, {position}, {length})",
            strpos="POSITION('{piece}' in {column})",
            floor="FLOOR({column})",
            random="RAND()",
            mod="mod({column}, {number})",
            decimate_number="FLOOR({column}/{number}) * {number}",
            string_between="SUBSTRING_INDEX(SUBSTRING_INDEX({column}, '{from_piece}', -1), '{to_piece}', 1)",
            split_part="SUBSTRING_INDEX(SUBSTRING_INDEX({column}, {delimiter}, {index}), {delimiter}, -1)",
            greatest="GREATEST({first_column}, {second_column})",
            least="LEAST({first_column}, {second_column})",
            sqrt="SQRT({column})",
            percentile_cont_window="PERCENTILE_CONT({percentile}) within group (ORDER BY {column}) over (partition by {group})",
            percentile_cont_all="PERCENTILE_CONT({percentile}) within group (ORDER BY {column})",
            ratio_to_report="{column} / NULLIF(SUM({column}) OVER (PARTITION BY {group}),0)",
            percent_of_total_all="{column} / NULLIF(SUM({column}) OVER (), 0)",
            row_number_empty="ROW_NUMBER() over ()",
            row_number_all="ROW_NUMBER() over (ORDER BY {order})",
            row_number_w_group="ROW_NUMBER() over (PARTITION by {group} ORDER BY {order})",
            string_agg="STRING_AGG({column}) over (PARTITION by {group} ORDER BY {order})",
            count_window="COUNT({column}) over (PARTITION by {group})",
            count_window_distinct="COUNT( DISTINCT {column}) over (PARTITION by {group})",
            sum_window="SUM({column}) over (PARTITION by {group})",
            max_window="MAX({column}) over (PARTITION by {group})",
            min_window="MIN({column}) over (PARTITION by {group})",
            count_window_all="COUNT({column}) over ()",
            count_window_distinct_all="COUNT(DISTINCT {column}) over ()",
            sum_window_all="SUM({column}) over ()",
            max_window_all="MAX({column}) over ()",
            min_window_all="MIN({column}) over ()",
            average_window="AVG(1.000*{column}) over (PARTITION by {group})",
            running_total="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            running_total_all="SUM({column}) over (ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            product_window="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (PARTITION BY {group}))",
            running_product_all="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (ORDER by {order} rows between unbounded preceding and current row))",
            running_product="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (PARTITION by {group} ORDER by {order} rows between unbounded preceding and current row))",
            moving_average="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS {window_size} PRECEDING)",
            lead="LEAD({column}) over (PARTITION by {group} ORDER BY {order})",
            lag="LAG({column}) over (PARTITION by {group} ORDER BY {order})",
            lead_ignore_nulls="LEAD({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order})",
            lag_ignore_nulls="LAG({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order})",
            dense_rank="DENSE_RANK() over (PARTITION by {group} ORDER BY {order})",
            dense_rank_all="DENSE_RANK() over ()",
            percent_rank="PERCENT_RANK() over (PARTITION by {group} ORDER BY {order})",
            lead_all="LEAD({column}) over (ORDER BY {order})",
            lag_all="LAG({column}) over (ORDER BY {order})",
            lead_offset="LEAD({column}, {offset}) over (PARTITION by {group} ORDER BY {order})",
            lag_offset="LAG({column}, {offset}) over (PARTITION by {group} ORDER BY {order})",
            lead_all_offset="LEAD({column}, {offset}) over (ORDER BY {order})",
            lag_all_offset="LAG({column}, {offset}) over (ORDER BY {order})",
            first_value_window="FIRST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            last_value_window="LAST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            first_value_window_before="FIRST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between  unbounded preceding and current row)",
            last_value_window_after="LAST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between current row and unbounded following)",
            first_value_all="FIRST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            last_value_all="LAST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            window_func_w_group_and_order="{function}({column}) over (PARTITION by {group} ORDER BY {order})",
            window_func_w_group_and_order_w_preceeding="{function}({column}) over (PARTITION by {group} ORDER BY {order} ROWS {window} PRECEDING)",
            window_func_w_group="{function}({column}) over (PARTITION by {group})",
            window_func_w_order="{function}({column}) over (ORDER by {order})",
            json_extract="JSON_VALUE({column}, '$.{key}')",
            single_column_function="{function}({column})",
            two_column_function="{function}({first_column}, {second_column})",
            string="{definition}",
        ),
        agg_functions=dict(
            count_all="COUNT(1)",
            count="COUNT({column})",
            count_distinct="COUNT(DISTINCT {column})",
            sum="SUM({column})",
            average="AVG(1.000 * {column})",
            max="MAX({column})",
            min="MIN({column})",
            stddev="STDDEV({column})",
            list_agg="STRING_AGG( {column}, ', ' ORDER BY ts)",
            median="PERCENTILE_CONT(.5) WITHIN GROUP (ORDER BY {column})",
            percentile_cont="PERCENTILE_CONT({percentile}) WITHIN GROUP (ORDER BY {column})",
            first_value="NULLIF(SUBSTRING(MIN(CONCAT(LEFT({base_column}, 19), COALESCE({column},''))),20, 1000),'')",
            last_value="NULLIF(SUBSTRING(MAX(CONCAT(LEFT({base_column}, 19), COALESCE({column},''))),20, 1000),'')",
            first_value_case="NULLIF(SUBSTRING(MIN(CASE WHEN {case} THEN CONCAT(LEFT({base_column}, 19), COALESCE({column},'')) END ),20, 1000),'')",
            last_value_case="NULLIF(SUBSTRING(MAX(CASE WHEN {case} THEN CONCAT(LEFT({base_column}, 19), COALESCE({column},'')) END ),20, 1000),'')",
        ),
    ),
    athena=dict(
        require_alias=False,
        reserved_words=[],
        value_quotes="'",
        column_quotes='"',
        cast_mapping=dict(
            date="DATE",
            timestamp="TIMESTAMP",
            string="STRING",
            text="STRING",
            bigint="BIGINT",
            integer="INTEGER",
            float="FLOAT",
            number="FLOAT",
            boolean="BOOL",
            structure="CAST({definition} AS {cast})",
        ),
        create_table_addons=["PARTITIONED BY ({partitioned_by}) ", "STORED AS PARQUET"],
        timezone_structure="convert_timezone('{timezone}', {definition})",
        operators=dict(
            contains="{left} ilike '%{right_value}%'",
            starts_with="{left} ilike '{right_value}%'",
            ends_with="{left} ilike '%{right_value}'",
            is_null="{left} is NULL",
            is_in="{left} in ({right}) ",
            greater_than="{left} > {right} ",
            less_than="{left} < {right} ",
            greater_than_equal="{left} >= {right} ",
            less_than_equal="{left} <= {right} ",
            equal="{left} = {right} ",
            is_empty="{left} = '' ",
            not_is_empty="{left} <> '' ",
            not_equal="{left} <> {right} ",
            not_contains="{left} not ilike '%{right_value}%'",
            not_starts_with="{left} not ilike '{right_value}%'",
            not_ends_with="{left} not ilike '%{right_value}'",
            not_is_null="{left} is not NULL",
            not_is_in="{left} not in ({right})",
        ),
        functions=dict(
            epoch_to_timestamp="timestamp 'epoch' + {column} * interval '1 second'",
            timestamp_to_epoch="extract('epoch' from {column})",
            now="SYSDATE",
            abs="ABS({column})",
            date_trunc="DATE_TRUNC('{datepart}', {column})",
            to_timezone="CONVERT_TIMEZONE({timezone}, {column})",
            date_add="DATE_ADD('{datepart}', {number}, {column})",
            date_diff="DATE_DIFF('{datepart}', {from_column}, {to_column})",
            date_diff_boundary="DATE_DIFF('{datepart}', {from_column}, {to_column})",
            date_to_now="DATE_DIFF('{datepart}', {from_column}, SYSDATE)",
            date_trunc_resolution="DATE_ADD('{datepart}', (floor(DATE_DIFF('{datepart}', '1990-01-01'::TIMESTAMP, {column})/{number})*{number})::INTEGER, '1990-01-01'::TIMESTAMP)",
            time_add="DATE_ADD('{datepart}', {number}, {column})",
            time_diff="DATE_DIFF('{datepart}', {from_column}, {to_column})",
            time_to_now="DATE_DIFF('{datepart}', {from_column}, SYSDATE)",
            time_trunc_resolution="DATE_ADD('{datepart}', (floor(DATE_DIFF('{datepart}', '1990-01-01'::TIMESTAMP, {column})/{number})*{number})::INTEGER, '1990-01-01'::TIMESTAMP)",
            date_part="DATE_PART('{datepart}', {column})",
            nullif="nullif({column}, {value_column})",
            exists="CASE WHEN {column} is not NULL THEN 1 ELSE 0 END",
            not_exists="CASE WHEN {column} is NULL THEN 1 ELSE 0 END",
            condition_flag="CASE WHEN {condition} THEN 1 ELSE 0 END",
            power="power({column}, {exponent_column})",
            nvl="NVL({first_column}, {second_column})",
            coalesce="COALESCE({first_column}, {second_column})",
            replace="REPLACE({column}, '{remove_string}', '{add_string}')",
            lower="LOWER({column})",
            length="LENGTH({column})",
            snake_case="LOWER(REPLACE({column}, ' ', '_'))",
            title="INITCAP(REPLACE({column}, '_', ' '))",
            concat="CONCAT({first_column}, {second_column})",
            substring="SUBSTRING({column}, {position}, {length})",
            strpos="STRPOS({column}, '{piece}')",
            random="random()",
            mod="mod({column}, {number})",
            decimate_number="FLOOR({column}/{number}) * {number}",
            floor="floor({column})",
            string_between="RTRIM(LTRIM(REGEXP_SUBSTR({column}, '{from_piece}([^{to_piece}]*){to_piece}'), '{from_piece}'),'{to_piece}')",
            regexp_extract="REGEXP_SUBSTR({column}, '{expression}')",
            regexp_count="REGEXP_COUNT({column},  '{expression}')",
            greatest="GREATEST({first_column}, {second_column})",
            least="LEAST({first_column}, {second_column})",
            sqrt="SQRT({column})",
            percentile_cont_window="PERCENTILE_CONT({percentile}) within group (ORDER BY {column}) over (partition by {group})",
            percentile_cont_all="PERCENTILE_CONT({percentile}) within group (ORDER BY {column}) over ()",
            ratio_to_report="RATIO_TO_REPORT({column}) OVER (PARTITION BY {group})",
            percent_of_total_all="RATIO_TO_REPORT({column}) OVER ()",
            row_number_empty="ROW_NUMBER() over ()",
            row_number_all="ROW_NUMBER() over (ORDER BY {order})",
            row_number_w_group="ROW_NUMBER() over (PARTITION by {group} ORDER BY {order})",
            string_agg="LISTAGG({column}) over (PARTITION by {group} ORDER BY {order})",
            count_window="COUNT({column}) over (PARTITION by {group})",
            count_window_distinct="COUNT( DISTINCT {column}) over (PARTITION by {group})",
            sum_window="SUM({column}) over (PARTITION by {group})",
            max_window="MAX({column}) over (PARTITION by {group})",
            min_window="MIN({column}) over (PARTITION by {group})",
            count_window_all="COUNT({column}) over ()",
            count_window_distinct_all="COUNT(DISTINCT {column}) over ()",
            sum_window_all="SUM({column}) over ()",
            max_window_all="MAX({column}) over ()",
            min_window_all="MIN({column}) over ()",
            average_window="AVG(1.000*{column}) over (PARTITION by {group})",
            running_total="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            running_total_all="SUM({column}) over (ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            product_window="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (PARTITION BY {group}))",
            running_product_all="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (ORDER by {order} rows between unbounded preceding and current row))",
            running_product="EXP(SUM(LN(CAST(nullif({column}, 0) as FLOAT))) over (PARTITION by {group} ORDER by {order} rows between unbounded preceding and current row))",
            moving_average="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS {window_size} PRECEDING)",
            moving_average_all="AVG(1.000*{column}) over (ORDER BY {order} ROWS {window_size} PRECEDING)",
            lead="LEAD({column}) over (PARTITION by {group} ORDER BY {order})",
            lag="LAG({column}) over (PARTITION by {group} ORDER BY {order})",
            lead_ignore_nulls="LEAD({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order})",
            lag_ignore_nulls="LAG({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order})",
            lead_all="LEAD({column}) over (ORDER BY {order})",
            lag_all="LAG({column}) over (ORDER BY {order})",
            first_value_window="FIRST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            last_value_window="LAST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            first_value_all="FIRST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            last_value_all="LAST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            json_extract="json_extract({column}, '$.{key}')",
            window_func_w_group_and_order="{function}({column}) over (PARTITION by {group} ORDER BY {order})",
            window_func_w_group_and_order_w_preceeding="{function}({column}) over (PARTITION by {group} ORDER BY {order} ROWS {window} PRECEDING)",
            window_func_w_group="{function}({column}) over (PARTITION by {group})",
            window_func_w_order="{function}({column}) over (ORDER by {order})",
            single_column_function="{function}({column})",
            two_column_function="{function}({first_column}, {second_column})",
            string="{definition}",
        ),
        agg_functions=dict(
            count_all="COUNT(1)",
            count="COUNT({column})",
            count_distinct="COUNT(DISTINCT {column})",
            sum="SUM({column})",
            average="AVG(1.0000*{column})",
            max="MAX({column})",
            min="MIN({column})",
            stddev="STDDEV({column})",
            list_agg="LISTAGG(DISTINCT {column}, ', ')",
            median="PERCENTILE_CONT(.5) WITHIN GROUP (ORDER BY {column})",
            percentile_cont="PERCENTILE_CONT({percentile}) WITHIN GROUP (ORDER BY {column})",
            first_value="NULLIF(SUBSTRING(MIN(CONCAT(LEFT({base_column}, 19), NVL({column},''))),20, 1000),'')",
            last_value="NULLIF(SUBSTRING(MAX(CONCAT(LEFT({base_column}, 19), NVL({column},''))),20, 1000),'')",
            first_value_case="NULLIF(SUBSTR(MIN(CASE WHEN {case} THEN CONCAT(CAST(TIMESTAMP_TRUNC({base_column}, SECOND) AS string), COALESCE({column},'') ) END), 23, 1000), '')",
            last_value_case="NULLIF(SUBSTR(MAX(CASE WHEN {case} THEN CONCAT(CAST(TIMESTAMP_TRUNC({base_column}, SECOND) AS string), coalesce({column},'')) END), 23, 1000), '')",
        ),
    ),
    clickhouse=dict(
        require_alias=False,
        reserved_words=[],
        value_quotes="'",
        column_quotes='"',
        cast_mapping=dict(
            date="DATE",
            timestamp="TIMESTAMP",
            string=f"VARCHAR({STRING_CHAR})",
            text="VARCHAR(65535)",
            bigint="BIGINT",
            integer="INTEGER",
            float="FLOAT",
            number="FLOAT",
            boolean="BOOL",
            structure="CAST({definition} AS {cast})",
        ),
        create_table_addons=[
            "DISTSTYLE {diststyle}",
            "DISTKEY({distkey})",
            "SORTKEY({sortkey})",
        ],
        timezone_structure="convert_timezone('{timezone}', {definition})",
        operators=dict(
            contains="{left} ilike '%{right_value}%'",
            starts_with="{left} ilike '{right_value}%'",
            ends_with="{left} ilike '%{right_value}'",
            is_null="{left} is NULL",
            is_in="{left} in ({right}) ",
            greater_than="{left} > {right} ",
            less_than="{left} < {right} ",
            greater_than_equal="{left} >= {right} ",
            less_than_equal="{left} <= {right} ",
            equal="{left} = {right} ",
            is_empty="{left} = '' ",
            not_is_empty="{left} <> '' ",
            not_equal="{left} <> {right} ",
            not_contains="{left} not ilike '%{right_value}%'",
            not_starts_with="{left} not ilike '{right_value}%'",
            not_ends_with="{left} not ilike '%{right_value}'",
            not_is_null="{left} is not NULL",
            not_is_in="{left} not in ({right})",
        ),
        functions=dict(
            epoch_to_timestamp="timestamp 'epoch' + {column} * interval '1 second'",
            timestamp_to_epoch="extract('epoch' from {column})",
            now="SYSDATE",
            abs="ABS({column})",
            date_trunc="DATE_TRUNC('{datepart}', {column})",
            date_add="DATE_ADD('{datepart}', {number}, {column})",
            date_diff="DATE_DIFF('{datepart}', {from_column}, {to_column})",
            date_to_now="DATE_DIFF('{datepart}', {from_column}, SYSDATE)",
            date_trunc_resolution="DATE_ADD('{datepart}', (floor(DATE_DIFF('{datepart}', '1990-01-01'::TIMESTAMP, {column})/{number})*{number})::INTEGER, '1990-01-01'::TIMESTAMP)",
            time_add="DATE_ADD('{datepart}', {number}, {column})",
            time_diff="DATE_DIFF('{datepart}', {from_column}, {to_column})",
            time_to_now="DATE_DIFF('{datepart}', {from_column}, SYSDATE)",
            time_trunc_resolution="DATE_ADD('{datepart}', (floor(DATE_DIFF('{datepart}', '1990-01-01'::TIMESTAMP, {column})/{number})*{number})::INTEGER, '1990-01-01'::TIMESTAMP)",
            date_part="DATE_PART('{datepart}', {column})",
            nullif="nullif({column}, {value_column})",
            exists="CASE WHEN {column} is not NULL THEN 1 ELSE 0 END",
            not_exists="CASE WHEN {column} is NULL THEN 1 ELSE 0 END",
            condition_flag="CASE WHEN {condition} THEN 1 ELSE 0 END",
            power="power({column}, {exponent_column})",
            nvl="NVL({first_column}, {second_column})",
            coalesce="COALESCE({first_column}, {second_column})",
            replace="REPLACE({column}, '{remove_string}', '{add_string}')",
            lower="LOWER({column})",
            length="LENGTH({column})",
            snake_case="LOWER(REPLACE({column}, ' ', '_'))",
            title="INITCAP(REPLACE({column}, '_', ' '))",
            concat="CONCAT({first_column}, {second_column})",
            substring="SUBSTRING({column}, {position}, {length})",
            strpos="STRPOS({column}, '{piece}')",
            random="random()",
            mod="mod({column}, {number})",
            decimate_number="FLOOR({column}/{number}) * {number}",
            floor="floor({column})",
            string_between="RTRIM(LTRIM(REGEXP_SUBSTR({column}, '{from_piece}([^{to_piece}]*){to_piece}'), '{from_piece}'),'{to_piece}')",
            regexp_extract="REGEXP_SUBSTR({column}, '{expression}')",
            regexp_count="REGEXP_COUNT({column},  '{expression}')",
            greatest="GREATEST({first_column}, {second_column})",
            least="LEAST({first_column}, {second_column})",
            sqrt="SQRT({column})",
            percentile_cont_window="PERCENTILE_CONT({percentile}) within group (ORDER BY {column}) over (partition by {group})",
            percentile_cont_all="PERCENTILE_CONT({percentile}) within group (ORDER BY {column}) over ()",
            ratio_to_report="RATIO_TO_REPORT({column}) OVER (PARTITION BY {group})",
            percent_of_total_all="RATIO_TO_REPORT({column}) OVER ()",
            row_number_empty="ROW_NUMBER() over ()",
            row_number_all="ROW_NUMBER() over (ORDER BY {order})",
            row_number_w_group="ROW_NUMBER() over (PARTITION by {group} ORDER BY {order})",
            string_agg="LISTAGG({column}) over (PARTITION by {group} ORDER BY {order})",
            count_window="COUNT({column}) over (PARTITION by {group})",
            count_window_distinct="COUNT( DISTINCT {column}) over (PARTITION by {group})",
            sum_window="SUM({column}) over (PARTITION by {group})",
            max_window="MAX({column}) over (PARTITION by {group})",
            min_window="MIN({column}) over (PARTITION by {group})",
            count_window_all="COUNT({column}) over ()",
            count_window_distinct_all="COUNT(DISTINCT {column}) over ()",
            sum_window_all="SUM({column}) over ()",
            max_window_all="MAX({column}) over ()",
            min_window_all="MIN({column}) over ()",
            average_window="AVG(1.000*{column}) over (PARTITION by {group})",
            running_total="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            running_total_all="SUM({column}) over (ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            product_window="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (PARTITION BY {group}))",
            running_product_all="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (ORDER by {order} rows between unbounded preceding and current row))",
            running_product="EXP(SUM(LN(CAST(nullif({column}, 0) as FLOAT))) over (PARTITION by {group} ORDER by {order} rows between unbounded preceding and current row))",
            moving_average="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS {window_size} PRECEDING)",
            moving_average_all="AVG(1.000*{column}) over (ORDER BY {order} ROWS {window_size} PRECEDING)",
            lead="LEAD({column}) over (PARTITION by {group} ORDER BY {order})",
            lag="LAG({column}) over (PARTITION by {group} ORDER BY {order})",
            lead_ignore_nulls="LEAD({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order})",
            lag_ignore_nulls="LAG({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order})",
            lead_all="LEAD({column}) over (ORDER BY {order})",
            lag_all="LAG({column}) over (ORDER BY {order})",
            first_value_window="FIRST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            last_value_window="LAST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            first_value_all="FIRST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            last_value_all="LAST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            json_extract="JSON_EXTRACT_PATH_TEXT({column}, '{key}')",
            window_func_w_group_and_order="{function}({column}) over (PARTITION by {group} ORDER BY {order})",
            window_func_w_group_and_order_w_preceeding="{function}({column}) over (PARTITION by {group} ORDER BY {order} ROWS {window} PRECEDING)",
            window_func_w_group="{function}({column}) over (PARTITION by {group})",
            window_func_w_order="{function}({column}) over (ORDER by {order})",
            single_column_function="{function}({column})",
            two_column_function="{function}({first_column}, {second_column})",
            string="{definition}",
        ),
        agg_functions=dict(
            count_all="COUNT(1)",
            count="COUNT({column})",
            count_distinct="COUNT(DISTINCT {column})",
            sum="SUM({column})",
            average="AVG(1.0000*{column})",
            max="MAX({column})",
            min="MIN({column})",
            stddev="STDDEV({column})",
            list_agg="LISTAGG(DISTINCT {column}, ', ')",
            median="PERCENTILE_CONT(.5) WITHIN GROUP (ORDER BY {column})",
            percentile_cont="PERCENTILE_CONT({percentile}) WITHIN GROUP (ORDER BY {column})",
            first_value="NULLIF(SUBSTRING(MIN(CONCAT(LEFT({base_column}, 19), NVL({column},''))),20, 1000),'')",
            last_value="NULLIF(SUBSTRING(MAX(CONCAT(LEFT({base_column}, 19), NVL({column},''))),20, 1000),'')",
            first_value_case="NULLIF(SUBSTR(MIN(CASE WHEN {case} THEN CONCAT(CAST(TIMESTAMP_TRUNC({base_column}, SECOND) AS string), COALESCE({column},'') ) END), 23, 1000), '')",
            last_value_case="NULLIF(SUBSTR(MAX(CASE WHEN {case} THEN CONCAT(CAST(TIMESTAMP_TRUNC({base_column}, SECOND) AS string), coalesce({column},'')) END ), 23, 1000), '')",
        ),
    ),
    druid=dict(
        require_alias=False,
        reserved_words=[],
        value_quotes="'",
        column_quotes='"',
        cast_mapping=dict(
            date="DATE",
            timestamp="TIMESTAMP",
            string=f"VARCHAR({STRING_CHAR})",
            text="VARCHAR(65535)",
            bigint="BIGINT",
            integer="INTEGER",
            float="FLOAT",
            number="FLOAT",
            boolean="BOOL",
            structure="CAST({definition} AS {cast})",
        ),
        create_table_addons=[
            "DISTSTYLE {diststyle}",
            "DISTKEY({distkey})",
            "SORTKEY({sortkey})",
        ],
        timezone_structure="convert_timezone('{timezone}', {definition})",
        operators=dict(
            contains="{left} ilike '%{right_value}%'",
            starts_with="{left} ilike '{right_value}%'",
            ends_with="{left} ilike '%{right_value}'",
            is_null="{left} is NULL",
            is_in="{left} in ({right}) ",
            greater_than="{left} > {right} ",
            less_than="{left} < {right} ",
            greater_than_equal="{left} >= {right} ",
            less_than_equal="{left} <= {right} ",
            equal="{left} = {right} ",
            is_empty="{left} = '' ",
            not_is_empty="{left} <> '' ",
            not_equal="{left} <> {right} ",
            not_contains="{left} not ilike '%{right_value}%'",
            not_starts_with="{left} not ilike '{right_value}%'",
            not_ends_with="{left} not ilike '%{right_value}'",
            not_is_null="{left} is not NULL",
            not_is_in="{left} not in ({right})",
        ),
        functions=dict(
            epoch_to_timestamp="timestamp 'epoch' + {column} * interval '1 second'",
            timestamp_to_epoch="extract('epoch' from {column})",
            now="SYSDATE",
            abs="ABS({column})",
            to_timezone="CONVERT_TIMEZONE({timezone}, {column})",
            date_trunc="DATE_TRUNC('{datepart}', {column})",
            date_add="DATE_ADD('{datepart}', {number}, {column})",
            date_diff="DATE_DIFF('{datepart}', {from_column}, {to_column})",
            date_to_now="DATE_DIFF('{datepart}', {from_column}, SYSDATE)",
            date_trunc_resolution="DATE_ADD('{datepart}', (floor(DATE_DIFF('{datepart}', '1990-01-01'::TIMESTAMP, {column})/{number})*{number})::INTEGER, '1990-01-01'::TIMESTAMP)",
            time_add="DATE_ADD('{datepart}', {number}, {column})",
            time_diff="DATE_DIFF('{datepart}', {from_column}, {to_column})",
            time_to_now="DATE_DIFF('{datepart}', {from_column}, SYSDATE)",
            time_trunc_resolution="DATE_ADD('{datepart}', (floor(DATE_DIFF('{datepart}', '1990-01-01'::TIMESTAMP, {column})/{number})*{number})::INTEGER, '1990-01-01'::TIMESTAMP)",
            date_part="DATE_PART('{datepart}', {column})",
            nullif="nullif({column}, {value_column})",
            exists="CASE WHEN {column} is not NULL THEN 1 ELSE 0 END",
            not_exists="CASE WHEN {column} is NULL THEN 1 ELSE 0 END",
            condition_flag="CASE WHEN {condition} THEN 1 ELSE 0 END",
            power="power({column}, {exponent_column})",
            nvl="NVL({first_column}, {second_column})",
            coalesce="COALESCE({first_column}, {second_column})",
            replace="REPLACE({column}, '{remove_string}', '{add_string}')",
            lower="LOWER({column})",
            length="LENGTH({column})",
            snake_case="LOWER(REPLACE({column}, ' ', '_'))",
            title="INITCAP(REPLACE({column}, '_', ' '))",
            concat="CONCAT({first_column}, {second_column})",
            substring="SUBSTRING({column}, {position}, {length})",
            strpos="STRPOS({column}, '{piece}')",
            random="random()",
            mod="mod({column}, {number})",
            decimate_number="FLOOR({column}/{number}) * {number}",
            floor="floor({column})",
            string_between="RTRIM(LTRIM(REGEXP_SUBSTR({column}, '{from_piece}([^{to_piece}]*){to_piece}'), '{from_piece}'),'{to_piece}')",
            regexp_extract="REGEXP_SUBSTR({column}, '{expression}')",
            regexp_count="REGEXP_COUNT({column},  '{expression}')",
            greatest="GREATEST({first_column}, {second_column})",
            least="LEAST({first_column}, {second_column})",
            sqrt="SQRT({column})",
            percentile_cont_window="PERCENTILE_CONT({percentile}) within group (ORDER BY {column}) over (partition by {group})",
            percentile_cont_all="PERCENTILE_CONT({percentile}) within group (ORDER BY {column}) over ()",
            ratio_to_report="RATIO_TO_REPORT({column}) OVER (PARTITION BY {group})",
            percent_of_total_all="RATIO_TO_REPORT({column}) OVER ()",
            row_number_empty="ROW_NUMBER() over ()",
            row_number_all="ROW_NUMBER() over (ORDER BY {order})",
            row_number_w_group="ROW_NUMBER() over (PARTITION by {group} ORDER BY {order})",
            string_agg="LISTAGG({column}) over (PARTITION by {group} ORDER BY {order})",
            count_window="COUNT({column}) over (PARTITION by {group})",
            count_window_distinct="COUNT( DISTINCT {column}) over (PARTITION by {group})",
            sum_window="SUM({column}) over (PARTITION by {group})",
            max_window="MAX({column}) over (PARTITION by {group})",
            min_window="MIN({column}) over (PARTITION by {group})",
            count_window_all="COUNT({column}) over ()",
            count_window_distinct_all="COUNT(DISTINCT {column}) over ()",
            sum_window_all="SUM({column}) over ()",
            max_window_all="MAX({column}) over ()",
            min_window_all="MIN({column}) over ()",
            average_window="AVG(1.000*{column}) over (PARTITION by {group})",
            running_total="SUM({column}) over (PARTITION by {group} ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            running_total_all="SUM({column}) over (ORDER BY {order} ROWS UNBOUNDED PRECEDING)",
            product_window="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (PARTITION BY {group}))",
            running_product_all="EXP(SUM(LN(CAST(nullif({column},0) as FLOAT))) over (ORDER by {order} rows between unbounded preceding and current row))",
            running_product="EXP(SUM(LN(CAST(nullif({column}, 0) as FLOAT))) over (PARTITION by {group} ORDER by {order} rows between unbounded preceding and current row))",
            moving_average="AVG(1.000*{column}) over (PARTITION by {group} ORDER BY {order} ROWS {window_size} PRECEDING)",
            moving_average_all="AVG(1.000*{column}) over (ORDER BY {order} ROWS {window_size} PRECEDING)",
            lead="LEAD({column}) over (PARTITION by {group} ORDER BY {order})",
            lag="LAG({column}) over (PARTITION by {group} ORDER BY {order})",
            lead_ignore_nulls="LEAD({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order})",
            lag_ignore_nulls="LAG({column}) IGNORE NULLS over (PARTITION by {group} ORDER BY {order})",
            lead_all="LEAD({column}) over (ORDER BY {order})",
            lag_all="LAG({column}) over (ORDER BY {order})",
            first_value_window="FIRST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            last_value_window="LAST_VALUE({column}) over (PARTITION by {group} ORDER BY {order} rows between unbounded preceding and unbounded following)",
            first_value_all="FIRST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            last_value_all="LAST_VALUE({column}) over (ORDER BY {order} rows between unbounded preceding and unbounded following)",
            json_extract="JSON_EXTRACT_PATH_TEXT({column}, '{key}')",
            window_func_w_group_and_order="{function}({column}) over (PARTITION by {group} ORDER BY {order})",
            window_func_w_group_and_order_w_preceeding="{function}({column}) over (PARTITION by {group} ORDER BY {order} ROWS {window} PRECEDING)",
            window_func_w_group="{function}({column}) over (PARTITION by {group})",
            window_func_w_order="{function}({column}) over (ORDER by {order})",
            single_column_function="{function}({column})",
            two_column_function="{function}({first_column}, {second_column})",
            string="{definition}",
        ),
        agg_functions=dict(
            count_all="COUNT(1)",
            count="COUNT({column})",
            count_distinct="COUNT(DISTINCT {column})",
            sum="SUM({column})",
            average="AVG(1.0000*{column})",
            max="MAX({column})",
            min="MIN({column})",
            stddev="STDDEV({column})",
            list_agg="LISTAGG(DISTINCT {column}, ', ')",
            median="PERCENTILE_CONT(.5) WITHIN GROUP (ORDER BY {column})",
            percentile_cont="PERCENTILE_CONT({percentile}) WITHIN GROUP (ORDER BY {column})",
            first_value="NULLIF(SUBSTRING(MIN(CONCAT(LEFT({base_column}, 19), NVL({column},''))),20, 1000),'')",
            last_value="NULLIF(SUBSTRING(MAX(CONCAT(LEFT({base_column}, 19), NVL({column},''))),20, 1000),'')",
            first_value_case="NULLIF(SUBSTR(MIN(CASE WHEN {case} THEN CONCAT(CAST(TIMESTAMP_TRUNC({base_column}, SECOND) AS string), COALESCE({column},'') ) END), 23, 1000), '')",
            last_value_case="NULLIF(SUBSTR(MAX(CASE WHEN {case} THEN CONCAT(CAST(TIMESTAMP_TRUNC({base_column}, SECOND) AS string), coalesce({column},'')) END ), 23, 1000), '')",
        ),
    ),
)

OPERATIONS = [
    "+",
    "-",
    "*",
    "/",
]  # , '>', '<', '=',' %' , '|', '=='] # characthers to find

TIMEZONES = [
    "Africa/Abidjan",
    "Africa/Accra",
    "Africa/Addis_Ababa",
    "Africa/Algiers",
    "Africa/Asmara",
    "Africa/Asmera",
    "Africa/Bamako",
    "Africa/Bangui",
    "Africa/Banjul",
    "Africa/Bissau",
    "Africa/Blantyre",
    "Africa/Brazzaville",
    "Africa/Bujumbura",
    "Africa/Cairo",
    "Africa/Casablanca",
    "Africa/Ceuta",
    "Africa/Conakry",
    "Africa/Dakar",
    "Africa/Dar_es_Salaam",
    "Africa/Djibouti",
    "Africa/Douala",
    "Africa/El_Aaiun",
    "Africa/Freetown",
    "Africa/Gaborone",
    "Africa/Harare",
    "Africa/Johannesburg",
    "Africa/Juba",
    "Africa/Kampala",
    "Africa/Khartoum",
    "Africa/Kigali",
    "Africa/Kinshasa",
    "Africa/Lagos",
    "Africa/Libreville",
    "Africa/Lome",
    "Africa/Luanda",
    "Africa/Lubumbashi",
    "Africa/Lusaka",
    "Africa/Malabo",
    "Africa/Maputo",
    "Africa/Maseru",
    "Africa/Mbabane",
    "Africa/Mogadishu",
    "Africa/Monrovia",
    "Africa/Nairobi",
    "Africa/Ndjamena",
    "Africa/Niamey",
    "Africa/Nouakchott",
    "Africa/Ouagadougou",
    "Africa/Porto-Novo",
    "Africa/Sao_Tome",
    "Africa/Timbuktu",
    "Africa/Tripoli",
    "Africa/Tunis",
    "Africa/Windhoek",
    "America/Adak",
    "America/Anchorage",
    "America/Anguilla",
    "America/Antigua",
    "America/Araguaina",
    "America/Argentina/Buenos_Aires",
    "America/Argentina/Catamarca",
    "America/Argentina/ComodRivadavia",
    "America/Argentina/Cordoba",
    "America/Argentina/Jujuy",
    "America/Argentina/La_Rioja",
    "America/Argentina/Mendoza",
    "America/Argentina/Rio_Gallegos",
    "America/Argentina/Salta",
    "America/Argentina/San_Juan",
    "America/Argentina/San_Luis",
    "America/Argentina/Tucuman",
    "America/Argentina/Ushuaia",
    "America/Aruba",
    "America/Asuncion",
    "America/Atikokan",
    "America/Atka",
    "America/Bahia",
    "America/Bahia_Banderas",
    "America/Barbados",
    "America/Belem",
    "America/Belize",
    "America/Blanc-Sablon",
    "America/Boa_Vista",
    "America/Bogota",
    "America/Boise",
    "America/Buenos_Aires",
    "America/Cambridge_Bay",
    "America/Campo_Grande",
    "America/Cancun",
    "America/Caracas",
    "America/Catamarca",
    "America/Cayenne",
    "America/Cayman",
    "America/Chicago",
    "America/Chihuahua",
    "America/Coral_Harbour",
    "America/Cordoba",
    "America/Costa_Rica",
    "America/Creston",
    "America/Cuiaba",
    "America/Curacao",
    "America/Danmarkshavn",
    "America/Dawson",
    "America/Dawson_Creek",
    "America/Denver",
    "America/Detroit",
    "America/Dominica",
    "America/Edmonton",
    "America/Eirunepe",
    "America/El_Salvador",
    "America/Ensenada",
    "America/Fort_Wayne",
    "America/Fortaleza",
    "America/Glace_Bay",
    "America/Godthab",
    "America/Goose_Bay",
    "America/Grand_Turk",
    "America/Grenada",
    "America/Guadeloupe",
    "America/Guatemala",
    "America/Guayaquil",
    "America/Guyana",
    "America/Halifax",
    "America/Havana",
    "America/Hermosillo",
    "America/Indiana/Indianapolis",
    "America/Indiana/Knox",
    "America/Indiana/Marengo",
    "America/Indiana/Petersburg",
    "America/Indiana/Tell_City",
    "America/Indiana/Vevay",
    "America/Indiana/Vincennes",
    "America/Indiana/Winamac",
    "America/Indianapolis",
    "America/Inuvik",
    "America/Iqaluit",
    "America/Jamaica",
    "America/Jujuy",
    "America/Juneau",
    "America/Kentucky/Louisville",
    "America/Kentucky/Monticello",
    "America/Knox_IN",
    "America/Kralendijk",
    "America/La_Paz",
    "America/Lima",
    "America/Los_Angeles",
    "America/Louisville",
    "America/Lower_Princes",
    "America/Maceio",
    "America/Managua",
    "America/Manaus",
    "America/Marigot",
    "America/Martinique",
    "America/Matamoros",
    "America/Mazatlan",
    "America/Mendoza",
    "America/Menominee",
    "America/Merida",
    "America/Metlakatla",
    "America/Mexico_City",
    "America/Miquelon",
    "America/Moncton",
    "America/Monterrey",
    "America/Montevideo",
    "America/Montreal",
    "America/Montserrat",
    "America/Nassau",
    "America/New_York",
    "America/Nipigon",
    "America/Nome",
    "America/Noronha",
    "America/North_Dakota/Beulah",
    "America/North_Dakota/Center",
    "America/North_Dakota/New_Salem",
    "America/Ojinaga",
    "America/Panama",
    "America/Pangnirtung",
    "America/Paramaribo",
    "America/Phoenix",
    "America/Port_of_Spain",
    "America/Port-au-Prince",
    "America/Porto_Acre",
    "America/Porto_Velho",
    "America/Puerto_Rico",
    "America/Rainy_River",
    "America/Rankin_Inlet",
    "America/Recife",
    "America/Regina",
    "America/Resolute",
    "America/Rio_Branco",
    "America/Rosario",
    "America/Santa_Isabel",
    "America/Santarem",
    "America/Santiago",
    "America/Santo_Domingo",
    "America/Sao_Paulo",
    "America/Scoresbysund",
    "America/Shiprock",
    "America/Sitka",
    "America/St_Barthelemy",
    "America/St_Johns",
    "America/St_Kitts",
    "America/St_Lucia",
    "America/St_Thomas",
    "America/St_Vincent",
    "America/Swift_Current",
    "America/Tegucigalpa",
    "America/Thule",
    "America/Thunder_Bay",
    "America/Tijuana",
    "America/Toronto",
    "America/Tortola",
    "America/Vancouver",
    "America/Virgin",
    "America/Whitehorse",
    "America/Winnipeg",
    "America/Yakutat",
    "America/Yellowknife",
    "Antarctica/Casey",
    "Antarctica/Davis",
    "Antarctica/DumontDUrville",
    "Antarctica/Macquarie",
    "Antarctica/Mawson",
    "Antarctica/McMurdo",
    "Antarctica/Palmer",
    "Antarctica/Rothera",
    "Antarctica/South_Pole",
    "Antarctica/Syowa",
    "Antarctica/Vostok",
    "Arctic/Longyearbyen",
    "Asia/Aden",
    "Asia/Almaty",
    "Asia/Amman",
    "Asia/Anadyr",
    "Asia/Aqtau",
    "Asia/Aqtobe",
    "Asia/Ashgabat",
    "Asia/Ashkhabad",
    "Asia/Baghdad",
    "Asia/Bahrain",
    "Asia/Baku",
    "Asia/Bangkok",
    "Asia/Beirut",
    "Asia/Bishkek",
    "Asia/Brunei",
    "Asia/Calcutta",
    "Asia/Choibalsan",
    "Asia/Chongqing",
    "Asia/Chungking",
    "Asia/Colombo",
    "Asia/Dacca",
    "Asia/Damascus",
    "Asia/Dhaka",
    "Asia/Dili",
    "Asia/Dubai",
    "Asia/Dushanbe",
    "Asia/Gaza",
    "Asia/Harbin",
    "Asia/Hebron",
    "Asia/Ho_Chi_Minh",
    "Asia/Hong_Kong",
    "Asia/Hovd",
    "Asia/Irkutsk",
    "Asia/Istanbul",
    "Asia/Jakarta",
    "Asia/Jayapura",
    "Asia/Jerusalem",
    # "Asia/Kabul",
    "Asia/Kamchatka",
    "Asia/Karachi",
    "Asia/Kashgar",
    # "Asia/Kathmandu",
    # "Asia/Katmandu",
    "Asia/Khandyga",
    # "Asia/Kolkata",
    "Asia/Krasnoyarsk",
    "Asia/Kuala_Lumpur",
    "Asia/Kuching",
    "Asia/Kuwait",
    "Asia/Macao",
    "Asia/Macau",
    "Asia/Magadan",
    "Asia/Makassar",
    "Asia/Manila",
    "Asia/Muscat",
    "Asia/Nicosia",
    "Asia/Novokuznetsk",
    "Asia/Novosibirsk",
    "Asia/Omsk",
    "Asia/Oral",
    "Asia/Phnom_Penh",
    "Asia/Pontianak",
    "Asia/Pyongyang",
    "Asia/Qatar",
    "Asia/Qyzylorda",
    # "Asia/Rangoon",
    "Asia/Riyadh",
    "Asia/Riyadh87",
    "Asia/Riyadh88",
    "Asia/Riyadh89",
    "Asia/Saigon",
    "Asia/Sakhalin",
    "Asia/Samarkand",
    "Asia/Seoul",
    "Asia/Shanghai",
    "Asia/Singapore",
    "Asia/Taipei",
    "Asia/Tashkent",
    "Asia/Tbilisi",
    "Asia/Tehran",
    "Asia/Tel_Aviv",
    "Asia/Thimbu",
    "Asia/Thimphu",
    "Asia/Tokyo",
    "Asia/Ujung_Pandang",
    "Asia/Ulaanbaatar",
    "Asia/Ulan_Bator",
    "Asia/Urumqi",
    "Asia/Ust-Nera",
    "Asia/Vientiane",
    "Asia/Vladivostok",
    "Asia/Yakutsk",
    "Asia/Yekaterinburg",
    "Asia/Yerevan",
    "Atlantic/Azores",
    "Atlantic/Bermuda",
    "Atlantic/Canary",
    "Atlantic/Cape_Verde",
    "Atlantic/Faeroe",
    "Atlantic/Faroe",
    "Atlantic/Jan_Mayen",
    "Atlantic/Madeira",
    "Atlantic/Reykjavik",
    "Atlantic/South_Georgia",
    "Atlantic/St_Helena",
    "Atlantic/Stanley",
    "Australia/ACT",
    "Australia/Adelaide",
    "Australia/Brisbane",
    "Australia/Broken_Hill",
    "Australia/Canberra",
    "Australia/Currie",
    "Australia/Darwin",
    "Australia/Eucla",
    "Australia/Hobart",
    "Australia/LHI",
    "Australia/Lindeman",
    "Australia/Lord_Howe",
    "Australia/Melbourne",
    "Australia/North",
    "Australia/NSW",
    "Australia/Perth",
    "Australia/Queensland",
    "Australia/South",
    "Australia/Sydney",
    "Australia/Tasmania",
    "Australia/Victoria",
    "Australia/West",
    "Australia/Yancowinna",
    "Brazil/Acre",
    "Brazil/DeNoronha",
    "Brazil/East",
    "Brazil/West",
    "Canada/Atlantic",
    "Canada/Central",
    "Canada/Eastern",
    "Canada/East-Saskatchewan",
    "Canada/Mountain",
    "Canada/Newfoundland",
    "Canada/Pacific",
    "Canada/Saskatchewan",
    "Canada/Yukon",
    "CET",
    "Chile/Continental",
    "Chile/EasterIsland",
    "CST6CDT",
    "Cuba",
    "EET",
    "Egypt",
    "Eire",
    "EST",
    "EST5EDT",
    "Etc/GMT",
    "Etc/GMT+0",
    "Etc/GMT+1",
    "Etc/GMT+10",
    "Etc/GMT+11",
    "Etc/GMT+12",
    "Etc/GMT+2",
    "Etc/GMT+3",
    "Etc/GMT+4",
    "Etc/GMT+5",
    "Etc/GMT+6",
    "Etc/GMT+7",
    "Etc/GMT+8",
    "Etc/GMT+9",
    "Etc/GMT0",
    "Etc/GMT-0",
    "Etc/GMT-1",
    "Etc/GMT-10",
    "Etc/GMT-11",
    "Etc/GMT-12",
    "Etc/GMT-13",
    "Etc/GMT-14",
    "Etc/GMT-2",
    "Etc/GMT-3",
    "Etc/GMT-4",
    "Etc/GMT-5",
    "Etc/GMT-6",
    "Etc/GMT-7",
    "Etc/GMT-8",
    "Etc/GMT-9",
    "Etc/Greenwich",
    "Etc/UCT",
    "Etc/Universal",
    "Etc/UTC",
    "Etc/Zulu",
    "Europe/Amsterdam",
    "Europe/Andorra",
    "Europe/Athens",
    "Europe/Belfast",
    "Europe/Belgrade",
    "Europe/Berlin",
    "Europe/Bratislava",
    "Europe/Brussels",
    "Europe/Bucharest",
    "Europe/Budapest",
    "Europe/Busingen",
    "Europe/Chisinau",
    "Europe/Copenhagen",
    "Europe/Dublin",
    "Europe/Gibraltar",
    "Europe/Guernsey",
    "Europe/Helsinki",
    "Europe/Isle_of_Man",
    "Europe/Istanbul",
    "Europe/Jersey",
    "Europe/Kaliningrad",
    "Europe/Kiev",
    "Europe/Lisbon",
    "Europe/Ljubljana",
    "Europe/London",
    "Europe/Luxembourg",
    "Europe/Madrid",
    "Europe/Malta",
    "Europe/Mariehamn",
    "Europe/Minsk",
    "Europe/Monaco",
    "Europe/Moscow",
    "Europe/Nicosia",
    "Europe/Oslo",
    "Europe/Paris",
    "Europe/Podgorica",
    "Europe/Prague",
    "Europe/Riga",
    "Europe/Rome",
    "Europe/Samara",
    "Europe/San_Marino",
    "Europe/Sarajevo",
    "Europe/Simferopol",
    "Europe/Skopje",
    "Europe/Sofia",
    "Europe/Stockholm",
    "Europe/Tallinn",
    "Europe/Tirane",
    "Europe/Tiraspol",
    "Europe/Uzhgorod",
    "Europe/Vaduz",
    "Europe/Vatican",
    "Europe/Vienna",
    "Europe/Vilnius",
    "Europe/Volgograd",
    "Europe/Warsaw",
    "Europe/Zagreb",
    "Europe/Zaporozhye",
    "Europe/Zurich",
    "GB",
    "GB-Eire",
    "GMT",
    "GMT+0",
    "GMT0",
    "GMT-0",
    "Greenwich",
    "Hongkong",
    "HST",
    "Iceland",
    "Indian/Antananarivo",
    "Indian/Chagos",
    "Indian/Christmas",
    "Indian/Cocos",
    "Indian/Comoro",
    "Indian/Kerguelen",
    "Indian/Mahe",
    "Indian/Maldives",
    "Indian/Mauritius",
    "Indian/Mayotte",
    "Indian/Reunion",
    "Iran",
    "Israel",
    "Jamaica",
    "Japan",
    "Kwajalein",
    "Libya",
    "MET",
    "Mexico/BajaNorte",
    "Mexico/BajaSur",
    "Mexico/General",
    "Mideast/Riyadh87",
    "Mideast/Riyadh88",
    "Mideast/Riyadh89",
    "MST",
    "MST7MDT",
    "Navajo",
    "NZ",
    "NZ-CHAT",
    "Pacific/Apia",
    "Pacific/Auckland",
    "Pacific/Chatham",
    "Pacific/Chuuk",
    "Pacific/Easter",
    "Pacific/Efate",
    "Pacific/Enderbury",
    "Pacific/Fakaofo",
    "Pacific/Fiji",
    "Pacific/Funafuti",
    "Pacific/Galapagos",
    "Pacific/Gambier",
    "Pacific/Guadalcanal",
    "Pacific/Guam",
    "Pacific/Honolulu",
    "Pacific/Johnston",
    "Pacific/Kiritimati",
    "Pacific/Kosrae",
    "Pacific/Kwajalein",
    "Pacific/Majuro",
    "Pacific/Marquesas",
    "Pacific/Midway",
    "Pacific/Nauru",
    "Pacific/Niue",
    "Pacific/Norfolk",
    "Pacific/Noumea",
    "Pacific/Pago_Pago",
    "Pacific/Palau",
    "Pacific/Pitcairn",
    "Pacific/Pohnpei",
    "Pacific/Ponape",
    "Pacific/Port_Moresby",
    "Pacific/Rarotonga",
    "Pacific/Saipan",
    "Pacific/Samoa",
    "Pacific/Tahiti",
    "Pacific/Tarawa",
    "Pacific/Tongatapu",
    "Pacific/Truk",
    "Pacific/Wake",
    "Pacific/Wallis",
    "Pacific/Yap",
    "Poland",
    "Portugal",
    "PRC",
    "PST8PDT",
    "ROK",
    "Singapore",
    "Turkey",
    "UCT",
    "Universal",
    "US/Alaska",
    "US/Aleutian",
    "US/Arizona",
    "US/Central",
    "US/Eastern",
    "US/East-Indiana",
    "US/Hawaii",
    "US/Indiana-Starke",
    "US/Michigan",
    "US/Mountain",
    "US/Pacific",
    "US/Pacific-New",
    "US/Samoa",
    "UTC",
    "WET",
    "W-SU",
    "Zulu",
]


MSSQL_TIMEZONE_MAPPING = {
    "+0000": "UTC",
    "+0100": "GMT Standard Time",
    "+0200": "Egypt Standard Time",
    "+0300": "FLE Standard Time",
    "+0400": "Iran Standard Time",
    "+0500": "India Standard Time",
    "+0600": "Omsk Standard Time",
    "+0700": "Altai Standard Time",
    "+0800": "China Standard Time",
    "+0900": "Tokyo Standard Time",
    "+1000": "Tasmania Standard Time",
    "+1100": "Russia Time Zone 10",
    "+1200": "UTC+12",
    "+1300": "UTC+13",
    "+1400": "Line Islands Standard Time",
    "-0100": "Cape Verde Standard Time",
    "-0200": "UTC-02",
    "-0300": "Bahia Standard Time",
    "-0400": "Cuba Standard Time",
    "-0500": "Central Standard Time",
    "-0600": "Mountain Standard Time",
    "-0700": "Pacific Standard Time",
    "-0800": "UTC-08",
    "-0900": "UTC-09",
    "-1000": "Hawaiian Standard Time",
    "-1100": "UTC-11",
    "-1200": "Dateline Standard Time",
}

RESOLUTIONS = ["second", "minute", "hour", "day", "week", "month", "quarter", "year"]


FUNCTIONS = [
    dict(
        name="contains",
        display_name="contains",
        kind="operators",
        output_type="boolean",
        description="checks if a string is contained in the columns",
        documentation="",
        input_fields=[
            dict(name="left", kind="column", data=["string"]),
            dict(name="right_value", kind="string", data=["string"]),
        ],
    ),
    dict(
        name="starts_with",
        display_name="starts_with",
        kind="operators",
        output_type="boolean",
        description="checks if a column starts with a string",
        documentation="",
        input_fields=[
            dict(name="left", kind="column", data=["string"]),
            dict(name="right_value", kind="string", data=["string"]),
        ],
    ),
    dict(
        name="ends_with",
        display_name="ends_with",
        kind="operators",
        output_type="boolean",
        description="checks if a column ends with a string",
        documentation="",
        input_fields=[
            dict(name="left", kind="column", data=["string"]),
            dict(name="right_value", kind="string", data=["string"]),
        ],
    ),
    dict(
        name="is_null",
        display_name="is_null",
        kind="operators",
        output_type="boolean",
        description="checks if column is NULL",
        documentation="",
        input_fields=[dict(name="left", kind="column", data=ALL_TYPES)],
    ),
    dict(
        name="is_in",
        display_name="is_in",
        kind="operators",
        output_type="boolean",
        description="Checks if column is in the set of values",
        documentation="",
        input_fields=[
            dict(name="left", kind="column", data=ALL_TYPES),
            dict(name="right", kind="list", data=ALL_TYPES),
        ],
    ),
    dict(
        name="greater_than",
        display_name="greater_than",
        kind="operators",
        output_type="boolean",
        description="Checks if column is greater than a value",
        documentation="",
        input_fields=[
            dict(name="left", kind="column", data=["number", "timestamp"]),
            dict(name="right", kind="column", data=["number", "timestamp"]),
        ],
    ),
    dict(
        name="less_than",
        display_name="less_than",
        kind="operators",
        output_type="boolean",
        description="Checks if column is less than a value",
        documentation="",
        input_fields=[
            dict(name="left", kind="column", data=["number", "timestamp"]),
            dict(name="right", kind="column", data=["number", "timestamp"]),
        ],
    ),
    dict(
        name="greater_than_equal",
        display_name="greater_than_equal",
        kind="operators",
        output_type="boolean",
        description="Checks if column is greater than or equal to value",
        documentation="",
        input_fields=[
            dict(name="left", kind="column", data=["number", "timestamp"]),
            dict(name="right", kind="column", data=["number", "timestamp"]),
        ],
    ),
    dict(
        name="less_than_equal",
        display_name="less_than_equal",
        kind="operators",
        output_type="boolean",
        description="Checks if column is less than or equal to value",
        documentation="",
        input_fields=[
            dict(name="left", kind="column", data=["number", "timestamp"]),
            dict(name="right", kind="column", data=["number", "timestamp"]),
        ],
    ),
    dict(
        name="equal",
        display_name="equal",
        kind="operators",
        output_type="boolean",
        description="Checks if column is equal to another column",
        documentation="",
        input_fields=[
            dict(name="left", kind="column", data=ALL_TYPES),
            dict(name="right", kind="column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="not_equal",
        display_name="not_equal",
        kind="operators",
        output_type="boolean",
        description="Checks if column is NOT equal to another column",
        documentation="",
        input_fields=[
            dict(name="left", kind="column", data=ALL_TYPES),
            dict(name="right", kind="column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="is_empty",
        display_name="is_empty",
        kind="operators",
        output_type="boolean",
        description="Checks if column is is empty",
        documentation="",
        input_fields=[
            dict(name="left", kind="column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="not_is_empty",
        display_name="not_is_empty",
        kind="operators",
        output_type="boolean",
        description="Checks if column is NOT empty ",
        documentation="",
        input_fields=[
            dict(name="left", kind="column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="not_contains",
        display_name="not_contains",
        kind="operators",
        output_type="boolean",
        description="Checks if string is NOT contained in a column",
        documentation="",
        input_fields=[
            dict(name="left", kind="column", data=["string"]),
            dict(name="right_value", kind="string", data=["string"]),
        ],
    ),
    dict(
        name="not_starts_with",
        display_name="not_starts_with",
        kind="operators",
        output_type="boolean",
        description="Checks if column does NOT start with a string",
        documentation="",
        input_fields=[
            dict(name="left", kind="column", data=["string"]),
            dict(name="right_value", kind="string", data=["string"]),
        ],
    ),
    dict(
        name="not_ends_with",
        display_name="not_ends_with",
        kind="operators",
        output_type="boolean",
        description="Checks if column does NOT end with a string",
        documentation="",
        input_fields=[
            dict(name="left", kind="column", data=["string"]),
            dict(name="right_value", kind="string", data=["string"]),
        ],
    ),
    dict(
        name="not_is_null",
        display_name="not_is_null",
        kind="operators",
        output_type="boolean",
        description="Checks if column is NOT NULL",
        documentation="",
        input_fields=[dict(name="left", kind="column", data=ALL_TYPES)],
    ),
    dict(
        name="not_is_in",
        display_name="not_is_in",
        kind="operators",
        output_type="boolean",
        description="Checks if column is NOT in an array of values",
        documentation="",
        input_fields=[
            dict(name="left", kind="column", data=ALL_TYPES),
            dict(name="right", kind="column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="contains_any",
        display_name="contains_any",
        kind="operators",
        output_type="boolean",
        description="checks of any of the words exist in the data",
        documentation="",
        input_fields=[
            dict(name="left", kind="column", data=ALL_TYPES),
            dict(name="right", kind="column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="not_contains_any",
        display_name="not_contains_any",
        kind="operators",
        output_type="boolean",
        description="checks of any of the words exist in the data",
        documentation="",
        input_fields=[
            dict(name="left", kind="column", data=ALL_TYPES),
            dict(name="right", kind="column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="to_json",
        display_name="to_json",
        kind="functions",
        output_type="column",
        description="(INTERNAL ONLY)",
        documentation="",
        input_fields=[
            dict(name="key_value_pair", kind="string", data=["string"]),
            dict(name="value_as_key", kind="string", data=["string"]),
            dict(name="json_pair", kind="string", data=["string"]),
        ],
    ),
    dict(
        name="iff",
        display_name="iff",
        kind="functions",
        output_type="column",
        description="IF this and only this, takes a condition and the true and false output",
        examples=[
            "group into buckets",
            "If the column is null then 1 else 0",
            "cluster",
            "condition",
        ],
        documentation="",
        input_fields=[
            dict(name="condition", kind="condition", data=None),
            dict(name="true_output", kind="column", data=ALL_TYPES),
            dict(name="false_output", kind="column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="field_bucket",
        display_name="field_bucket",
        kind="functions",
        output_type="column",
        description="Buckets the column based on the field object",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="field", kind="string", data=["string"]),
        ],
    ),
    dict(
        name="is_conv",
        display_name="is_conv",
        kind="functions",
        output_type="number",
        description="Creates a 1/0 column based on the timestamp being not null and being after all the other columns",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["timestamp"]),
            dict(name="condition_on_columns", kind="list_column", data=["timestamp"]),
        ],
    ),
    dict(
        name="epoch_to_timestamp",
        display_name="Epoch to Timestamp",
        kind="functions",
        output_type="timestamp",
        description="Converts EPoch time to timestamp",
        documentation="",
        input_fields=[dict(name="column", kind="column", data=["number"])],
    ),
    dict(
        name="abs",
        display_name="ABS",
        kind="functions",
        output_type="number",
        description="Gets the absolute value of a number",
        documentation="",
        examples=["absolute value"],
        input_fields=[dict(name="column", kind="column", data=["number"])],
    ),
    dict(
        name="now",
        display_name="now",
        kind="functions",
        output_type="timestamp",
        description="Current timestamp in UTC",
        documentation="",
        input_fields=[],
    ),
    dict(
        name="local_now",
        display_name="local_now",
        kind="functions",
        output_type="timestamp",
        description="Current timestamp in local timezone",
        documentation="",
        input_fields=[],
    ),
    dict(
        name="to_timezone",
        display_name="to_timezone",
        kind="functions",
        output_type="timestamp",
        description="Current timestamp in desried timezone timezone",
        documentation="",
        input_fields=[
            dict(name="timezone", kind="column", data=["string"]),
            dict(name="column", kind="column", data=["timestamp"]),
        ],
    ),
    dict(
        name="date_trunc",
        display_name="Truncate Time",
        kind="functions",
        output_type="timestamp",
        description="Truncates a column to the nearest resolution.",
        documentation="""This will take a time column like 2019-03-02 and make it to the beginning of the resolution.\nIf Month then it will be 2019-03-01""",
        input_fields=[
            dict(name="datepart", kind="constraint_list", data=RESOLUTIONS),
            dict(name="column", kind="column", data=["timestamp"]),
        ],
    ),
    dict(
        name="date_add",
        display_name="Add To Date",
        kind="functions",
        output_type="timestamp",
        description="Adds a time interval with a specific resolution to a timestamp",
        documentation="",
        input_fields=[
            dict(name="datepart", kind="constraint_list", data=RESOLUTIONS),
            dict(name="number", kind="integer", data=["number"]),
            dict(name="column", kind="column", data=["timestamp"]),
        ],
    ),
    dict(
        name="date_diff",
        display_name="Diff Date",
        kind="functions",
        output_type="integer",
        description="Gets the resolution (day, week, ..) between 2 timestamp",
        documentation="",
        input_fields=[
            dict(name="datepart", kind="constraint_list", data=RESOLUTIONS),
            dict(name="from_column", kind="column", data=["timestamp"]),
            dict(name="to_column", kind="column", data=["timestamp"]),
        ],
    ),
    dict(
        name="date_diff_boundary",
        display_name="Diff Date using Boundary",
        kind="functions",
        output_type="integer",
        description="Gets the boundary crosses (day, week, ..) between 2 timestamp",
        documentation="",
        input_fields=[
            dict(name="datepart", kind="constraint_list", data=RESOLUTIONS),
            dict(name="from_column", kind="column", data=["timestamp"]),
            dict(name="to_column", kind="column", data=["timestamp"]),
        ],
    ),
    dict(
        name="date_to_now",
        display_name="Date to Now",
        kind="functions",
        output_type="integer",
        description="Gets the Days, Weeks,... from a column to NOW",
        documentation="",
        input_fields=[
            dict(name="datepart", kind="constraint_list", data=RESOLUTIONS),
            dict(name="from_column", kind="column", data=["timestamp"]),
        ],
    ),
    dict(
        name="time_add",
        display_name="Add To Time",
        kind="functions",
        output_type="timestamp",
        description="Adds a time interval to a timestamp",
        documentation="",
        input_fields=[
            dict(name="datepart", kind="constraint_list", data=RESOLUTIONS),
            dict(name="number", kind="integer", data=["number"]),
            dict(name="column", kind="column", data=["timestamp"]),
        ],
    ),
    dict(
        name="time_diff",
        display_name="Diff Time",
        kind="functions",
        output_type="integer",
        description="Gets the resolution (day, week, ..) between 2 timestamp",
        documentation="",
        input_fields=[
            dict(name="datepart", kind="constraint_list", data=RESOLUTIONS),
            dict(name="from_column", kind="column", data=["timestamp"]),
            dict(name="to_column", kind="column", data=["timestamp"]),
        ],
    ),
    dict(
        name="is_time_to_date",
        display_name="is_time_to_date",
        kind="functions",
        output_type="boolean",
        description="Returns the last value in the data",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["timestamp"]),
            dict(name="datepart", kind="constraint_list", data=RESOLUTIONS),
        ],
    ),
    dict(
        name="is_last_time_to_date",
        display_name="is_last_time_to_date",
        kind="functions",
        output_type="boolean",
        description="Returns the last value in the data",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["timestamp"]),
            dict(name="datepart", kind="constraint_list", data=RESOLUTIONS),
        ],
    ),
    dict(
        name="time_to_now",
        display_name="time_to_now",
        kind="functions",
        output_type="integer",
        description="Gets the Days, Weeks,... from a column to NOW",
        documentation="",
        input_fields=[
            dict(name="datepart", kind="constraint_list", data=RESOLUTIONS),
            dict(name="from_column", kind="column", data=["timestamp"]),
        ],
    ),
    dict(
        name="date_trunc_resolution",
        display_name="Truncate DATE Bucket",
        kind="functions",
        output_type="timestamp",
        description="Truncates the timestamp by a bucketed resolution i.e. every 5 minutes",
        documentation="",
        input_fields=[
            dict(name="datepart", kind="constraint_list", data=RESOLUTIONS),
            dict(name="column", kind="column", data=["timestamp"]),
            dict(name="number", kind="integer", data=["number"]),
        ],
    ),
    dict(
        name="time_trunc_resolution",
        display_name="Truncate Time Bucket",
        kind="functions",
        output_type="timestamp",
        description="Truncates the timestamp by a bucketed resolution i.e. every 5 minutes",
        documentation="",
        input_fields=[
            dict(name="datepart", kind="constraint_list", data=RESOLUTIONS),
            dict(name="column", kind="column", data=["timestamp"]),
            dict(name="number", kind="integer", data=["number"]),
        ],
    ),
    dict(
        name="date_part",
        display_name="date_part",
        kind="functions",
        output_type="number",
        description="Gets the part of a date (i.e. dow, month, ...)",
        documentation="",
        input_fields=[
            dict(name="datepart", kind="constraint_list", data=RESOLUTIONS),
            dict(name="column", kind="column", data=["timestamp"]),
        ],
    ),
    dict(
        name="nullif",
        display_name="nullif",
        kind="functions",
        output_type="column",
        description="Returns null if the column matches the value",
        documentation="",
        examples=["null if"],
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="value_column", kind="column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="exists",
        display_name="exists",
        kind="functions",
        output_type="integer",
        description="Returns 1 if the column is not NULL",
        documentation="",
        input_fields=[dict(name="column", kind="column", data=ALL_TYPES)],
    ),
    dict(
        name="not_exists",
        display_name="exists",
        kind="functions",
        output_type="integer",
        description="Returns 1 if the column is NULL",
        documentation="",
        input_fields=[dict(name="column", kind="column", data=ALL_TYPES)],
    ),
    dict(
        name="condition_flag",
        display_name="exists",
        kind="functions",
        output_type="integer",
        description="Returns 1 if the condition is met",
        documentation="",
        input_fields=[dict(name="condition", kind="condition", data=ALL_TYPES)],
    ),
    dict(
        name="power",
        display_name="power",
        kind="functions",
        output_type="column",
        description="Raise a column to the exponential",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="exponent_column", kind="column", data=["number"]),
        ],
    ),
    dict(
        name="nvl",
        display_name="nvl",
        kind="functions",
        output_type="column",
        description="Returns the first column if it is not null else returns second column",
        documentation="",
        input_fields=[
            dict(name="first_column", kind="column", data=ALL_TYPES),
            dict(name="second_column", kind="column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="coalesce",
        display_name="coalesce",
        kind="functions",
        output_type="column",
        description="Returns the first column if it is not null else returns second column",
        documentation="",
        input_fields=[
            dict(name="first_column", kind="column", data=ALL_TYPES),
            dict(name="second_column", kind="column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="replace",
        display_name="replace",
        kind="functions",
        output_type="string",
        description="Replaces a string in a column with a different string",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["string"]),
            dict(name="remove_string", kind="string", data=["string"]),
            dict(name="add_string", kind="string", data=["string"]),
        ],
    ),
    dict(
        name="lower",
        display_name="lower",
        kind="functions",
        output_type="string",
        description="lowers a column",
        documentation="",
        input_fields=[dict(name="column", kind="column", data=["string"])],
    ),
    dict(
        name="length",
        display_name="length",
        kind="functions",
        output_type="integer",
        description="finds the length of column",
        documentation="",
        input_fields=[dict(name="column", kind="column", data=["string"])],
    ),
    dict(
        name="snake_case",
        display_name="snake_case",
        kind="functions",
        output_type="string",
        description="snake cases a column",
        documentation="",
        input_fields=[dict(name="column", kind="column", data=["string"])],
    ),
    dict(
        name="title",
        display_name="title",
        kind="functions",
        output_type="string",
        description="title cases a column",
        documentation="",
        input_fields=[dict(name="column", kind="column", data=["string"])],
    ),
    dict(
        name="concat",
        display_name="concat",
        kind="functions",
        output_type="string",
        description="Concatenates 2 columns",
        documentation="",
        input_fields=[
            dict(name="first_column", kind="column", data=["string"]),
            dict(name="second_column", kind="column", data=["string"]),
        ],
    ),
    dict(
        name="substring",
        display_name="substring",
        kind="functions",
        output_type="string",
        description="Get a piece of string that starts at a position and is length long",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["string"]),
            dict(name="position", kind="integer", data=["number"]),
            dict(name="length", kind="integer", data=["number"]),
        ],
    ),
    dict(
        name="strpos",
        display_name="strpos",
        kind="functions",
        output_type="integer",
        description="Finds the position of a string in a column",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["string"]),
            dict(name="piece", kind="string", data=["string"]),
        ],
    ),
    dict(
        name="floor",
        display_name="floor",
        kind="functions",
        output_type="integer",
        description="rounds a number down",
        documentation="",
        input_fields=[dict(name="column", kind="column", data=["number"])],
    ),
    dict(
        name="mod",
        display_name="mod",
        kind="functions",
        output_type="integer",
        description="Returns the remainder of a column",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="number", kind="integer", data=["number"]),
        ],
    ),
    dict(
        name="random",
        display_name="Random",
        kind="functions",
        output_type="float",
        description="Generates a random number from 0 to 1",
        documentation="",
        input_fields=[],
    ),
    dict(
        name="decimate_number",
        display_name="decimate_number",
        kind="functions",
        output_type="float",
        description="Buckets a number (i.e. bucket money by $5 groups)",
        examples=["bucket by 5", "decimate by ", "cluster values"],
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="number", kind="integer", data=["number"]),
        ],
    ),
    dict(
        name="string_between",
        display_name="string_between",
        kind="functions",
        output_type="string",
        description="Finds the string in the column between 2 strings",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["string"]),
            dict(name="from_piece", kind="string", data=["string"]),
            dict(name="to_piece", kind="string", data=["string"]),
        ],
    ),
    dict(
        name="split_part",
        display_name="split_part",
        kind="functions",
        output_type="string",
        description="splits a string and returns the part (index starts at 1)",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["string"]),
            dict(name="delimiter", kind="string", data=["string"]),
            dict(name="index", kind="integer", data=["number"]),
        ],
    ),
    dict(
        name="regexp_extract",
        display_name="regexp_extract",
        kind="functions",
        output_type="string",
        description="Returns the characters extracted from a string by searching for a regular expression pattern",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["string"]),
            dict(name="expression", kind="string", data=["string"]),
        ],
    ),
    dict(
        name="regexp_count",
        display_name="regexp_count",
        kind="functions",
        output_type="number",
        description="Returns the number of characters extracted from a string by searching for a regular expression pattern",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["string"]),
            dict(name="expression", kind="string", data=["string"]),
        ],
    ),
    dict(
        name="greatest",
        display_name="greatest",
        kind="functions",
        output_type="float",
        description="Finds the MAX between the 2 columns",
        documentation="",
        input_fields=[
            dict(name="first_column", kind="column", data=ALL_TYPES),
            dict(name="second_column", kind="column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="least",
        display_name="least",
        kind="functions",
        output_type="float",
        description="Finds the MIN between the 2 columns",
        documentation="",
        input_fields=[
            dict(name="first_column", kind="column", data=ALL_TYPES),
            dict(name="second_column", kind="column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="sqrt",
        display_name="sqrt",
        kind="functions",
        output_type="float",
        description="Finds the SQRT of a column",
        documentation="",
        input_fields=[dict(name="column", kind="column", data=["number"])],
    ),
    dict(
        name="percentile_cont_window",
        display_name="Percentile Cont Window",
        kind="functions",
        output_type="float",
        description="Compute the Continuous percentile for a window ",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="percentile", kind="float", data=["number"]),
            dict(name="group", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="percentile_cont_all",
        display_name="Percentile Cont All",
        kind="functions",
        output_type="float",
        description="Compute the Continuous percentile for a window ",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="percentile", kind="float", data=["number"]),
        ],
    ),
    dict(
        name="ratio_to_report",
        display_name="ratio_to_report",
        kind="functions",
        output_type="float",
        description="Computes the SUM divided by the COUNT(*)  within a group ",
        documentation="",
        examples=["ratio to report", "percent of total"],
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="group", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="percent_of_total_all",
        display_name="percent_of_total_all",
        kind="functions",
        output_type="float",
        description="Computes the SUM divided by the COUNT(*) for the entire dataset",
        documentation="",
        input_fields=[dict(name="column", kind="column", data=["number"])],
    ),
    dict(
        name="row_number_empty",
        display_name="row_number_raw",
        kind="functions",
        output_type="integer",
        description="computes the row number of the data (random)",
        documentation="",
        input_fields=[],
    ),
    dict(
        name="row_number_all",
        display_name="row_number",
        kind="functions",
        output_type="integer",
        description="computes the row number ordered by order",
        documentation="",
        input_fields=[dict(name="order", kind="list_column", data=ALL_TYPES)],
    ),
    dict(
        name="row_number_w_group",
        display_name="row_number_w_group",
        kind="functions",
        output_type="integer",
        description="Computes the row number within a group ordered by order",
        documentation="",
        input_fields=[
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="string_agg",
        display_name="string_agg",
        kind="functions",
        output_type="string",
        description="Combines the string ",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["string"]),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="count_window",
        display_name="count_window",
        kind="functions",
        output_type="integer",
        description="count the total rows in a group",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="count_window_distinct",
        display_name="count_window_distinct",
        kind="functions",
        output_type="integer",
        description="count the total rows in a group",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="sum_window",
        display_name="sum_window",
        kind="functions",
        output_type="float",
        description="sums the values in a group",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="group", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="max_window",
        display_name="max_window",
        kind="functions",
        output_type="column",
        description="MAX of the values in a group",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="min_window",
        display_name="min_window",
        kind="functions",
        output_type="column",
        description="MIN of the values in a group",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="sum_window_all",
        display_name="sum_window_all",
        kind="functions",
        output_type="float",
        description="sums the values",
        documentation="",
        input_fields=[dict(name="column", kind="column", data=["number"])],
    ),
    dict(
        name="count_window_all",
        display_name="count_window_all",
        kind="functions",
        output_type="float",
        description="Count over the entire dataset",
        documentation="",
        input_fields=[dict(name="column", kind="column", data=ALL_TYPES)],
    ),
    dict(
        name="count_window_distinct_all",
        display_name="count_window_distinct_all",
        kind="functions",
        output_type="float",
        description="Count DISTINCT over the entire dataset",
        documentation="",
        input_fields=[dict(name="column", kind="column", data=ALL_TYPES)],
    ),
    dict(
        name="min_window_all",
        display_name="min_window_all",
        kind="functions",
        output_type="float",
        description="MIN over the entire dataset",
        documentation="",
        input_fields=[dict(name="column", kind="column", data=ALL_TYPES)],
    ),
    dict(
        name="max_window_all",
        display_name="max_window_all",
        kind="functions",
        output_type="float",
        description="MAX over the entire dataset",
        documentation="",
        input_fields=[dict(name="column", kind="column", data=ALL_TYPES)],
    ),
    dict(
        name="count_window_before",
        display_name="count_window_before",
        kind="functions",
        output_type="integer",
        description="count the total rows in a group before the current row",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="count_window_distinct_before",
        display_name="count_window_distinct_before",
        kind="functions",
        output_type="integer",
        description="count the total rows in a group before the current row",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="sum_window_before",
        display_name="sum_window_before",
        kind="functions",
        output_type="float",
        description="sums the values in a group before the current row",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="max_window_before",
        display_name="max_window_before",
        kind="functions",
        output_type="column",
        description="MAX of the values in a group before the current row",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="min_window_before",
        display_name="min_window_before",
        kind="functions",
        output_type="column",
        description="MIN of the values in a group before",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="average_window_before",
        display_name="average_window_before",
        kind="functions",
        output_type="float",
        description="averages the values in a group before the current row",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="count_window_after",
        display_name="count_window_after",
        kind="functions",
        output_type="integer",
        description="count the total rows in a group after the current row",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="count_window_distinct_after",
        display_name="count_window_distinct_after",
        kind="functions",
        output_type="integer",
        description="count the total rows in a group after the current row",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="sum_window_after",
        display_name="sum_window_after",
        kind="functions",
        output_type="float",
        description="sums the values in a group after the current row",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="max_window_after",
        display_name="max_window_after",
        kind="functions",
        output_type="column",
        description="MAX of the values in a group after the current row",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="min_window_after",
        display_name="min_window_after",
        kind="functions",
        output_type="column",
        description="MIN of the values in a group after",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="average_window_after",
        display_name="average_window_after",
        kind="functions",
        output_type="float",
        description="averages the values in a group after the current row",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="average_window",
        display_name="average_window",
        kind="functions",
        output_type="float",
        description="averages the values in a group",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="group", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="running_total",
        display_name="running_total",
        kind="functions",
        output_type="float",
        description="Sums all the rows within the group in the declared order",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="running_total_all",
        display_name="running_total_all",
        kind="functions",
        output_type="float",
        description="Sums all the rows in the declared order",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="product_window",
        display_name="product_window",
        kind="functions",
        output_type="float",
        description="Computes the product of all column in a group",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="group", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="running_product_all",
        display_name="running_product_all",
        kind="functions",
        output_type="float",
        description="Computes the product in all the rows before this row",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="running_product",
        display_name="running_product",
        kind="functions",
        output_type="float",
        description="Computes the product in all the rows before this row",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="moving_average",
        display_name="moving_average",
        kind="functions",
        output_type="float",
        description="Computes the rollowing average in the group in a window ordered by window_size",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
            dict(name="window_size", kind="integer", data=["number"]),
        ],
    ),
    dict(
        name="moving_average_all",
        display_name="moving_average_all",
        kind="functions",
        output_type="float",
        description="Computes the rollowing average in a window ordered by window_size",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="order", kind="list_column", data=ALL_TYPES),
            dict(name="window_size", kind="integer", data=ALL_TYPES),
        ],
    ),
    dict(
        name="lead",
        display_name="lead",
        kind="functions",
        output_type="column",
        description="Returns the row after the current row within a group",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="lag",
        display_name="lag",
        kind="functions",
        output_type="column",
        description="Returns the row before the current row within a group",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="lead_ignore_nulls",
        display_name="lead_ignore_nulls",
        kind="functions",
        output_type="column",
        description="Returns the row after the current row within a group (IGNORES NULLS)",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="lag_ignore_nulls",
        display_name="lag_ignore_nulls",
        kind="functions",
        output_type="column",
        description="Returns the row before the current row within a group (IGNORES NULLS)",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="dense_rank",
        display_name="dense_rank",
        kind="functions",
        output_type="number",
        description="Returns the row before the current row within a group",
        documentation="",
        input_fields=[
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="dense_rank_all",
        display_name="dense_rank_all",
        kind="functions",
        output_type="number",
        description="Returns the row before the current row",
        documentation="",
        input_fields=[],
    ),
    dict(
        name="percent_rank",
        display_name="percent_rank",
        kind="functions",
        output_type="number",
        description="Returns the row before the current row within a group",
        documentation="",
        input_fields=[
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="lead_all",
        display_name="lead_all",
        kind="functions",
        output_type="column",
        description="Returns the row after the current row",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="lag_all",
        display_name="lag_all",
        kind="functions",
        output_type="column",
        description="Returns the row before the current row",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="lead_offset",
        display_name="lead",
        kind="functions",
        output_type="column",
        description="Returns the row after the current row within a group",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="offset", kind="integer", data=["number"]),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="lag_offset",
        display_name="lag",
        kind="functions",
        output_type="column",
        description="Returns the row before the current row within a group",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="offset", kind="integer", data=["number"]),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="lead_all_offset",
        display_name="lead_all",
        kind="functions",
        output_type="column",
        description="Returns the row after the current row",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="offset", kind="integer", data=["number"]),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="lag_all_offset",
        display_name="lag_all",
        kind="functions",
        output_type="column",
        description="Returns the row before the current row",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="offset", kind="integer", data=["number"]),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="first_value_window",
        display_name="first_value_window",
        kind="functions",
        output_type="column",
        description="Returns the first row in the window",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="last_value_window",
        display_name="last_value_window",
        kind="functions",
        output_type="column",
        description="Returns the last value in the window",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="first_value_window_before",
        display_name="first_value_window_before",
        kind="functions",
        output_type="column",
        description="Returns the first row in the window BEFORE the current row",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="last_value_window_after",
        display_name="last_value_window_after",
        kind="functions",
        output_type="column",
        description="Returns the last value in the window AFTER the current row",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="first_value_all",
        display_name="first_value_all",
        kind="functions",
        output_type="column",
        description="Returns the first row in the window in data",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="last_value_all",
        display_name="last_value_all",
        kind="functions",
        output_type="column",
        description="Returns the last value in the data",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="json_extract",
        display_name="json_extract",
        kind="functions",
        output_type="column",
        description="Parses a json column to return the value ",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="key", kind="string", data=["string"]),
        ],
    ),
    dict(
        name="window_func_w_group_and_order",
        display_name="window_func_w_group_and_order",
        kind="functions",
        output_type="string",
        description="freehand SQL function (EXPERT ONLY)",
        documentation="",
        input_fields=[
            dict(name="function", kind="string", data=["string"]),
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="window_func_w_group_and_order_w_preceeding",
        display_name="window_func_w_group_and_order_w_preceeding",
        kind="functions",
        output_type="string",
        description="freehand SQL function (EXPERT ONLY)",
        documentation="",
        input_fields=[
            dict(name="function", kind="string", data=["string"]),
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
            dict(name="window", kind="integer", data=ALL_TYPES),
        ],
    ),
    dict(
        name="window_func_w_group",
        display_name="window_func_w_group",
        kind="functions",
        output_type="string",
        description="freehand SQL function (EXPERT ONLY)",
        documentation="",
        input_fields=[
            dict(name="function", kind="string", data=["string"]),
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="group", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="window_func_w_order",
        display_name="window_func_w_order",
        kind="functions",
        output_type="string",
        description="freehand SQL function (EXPERT ONLY)",
        documentation="",
        input_fields=[
            dict(name="function", kind="string", data=["string"]),
            dict(name="column", kind="column", data=ALL_TYPES),
            dict(name="order", kind="list_column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="single_column_function",
        display_name="single_column_function",
        kind="functions",
        output_type="string",
        description="freehand SQL function (EXPERT ONLY)",
        documentation="",
        input_fields=[
            dict(name="function", kind="string", data=["string"]),
            dict(name="column", kind="column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="two_column_function",
        display_name="two_column_function",
        kind="functions",
        output_type="string",
        description="freehand SQL function (EXPERT ONLY)",
        documentation="",
        input_fields=[
            dict(name="function", kind="string", data=["string"]),
            dict(name="first_column", kind="column", data=ALL_TYPES),
            dict(name="second_column", kind="column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="count_all",
        display_name="Count Records",
        kind="agg_functions",
        output_type="integer",
        description="Counts the number of rows",
        documentation="",
        input_fields=[],
    ),
    dict(
        name="count",
        display_name="Count",
        kind="agg_functions",
        output_type="integer",
        description="Counts the total non null values in the column",
        documentation="",
        input_fields=[
            dict(
                name="column",
                kind="column",
                data=ALL_TYPES,
            )
        ],
    ),
    dict(
        name="count_distinct",
        display_name="Count Distinct",
        kind="agg_functions",
        output_type="integer",
        description="Counts the distict values in the column",
        documentation="",
        input_fields=[
            dict(
                name="column",
                kind="column",
                data=ALL_TYPES,
            )
        ],
    ),
    dict(
        name="sum",
        display_name="Sum",
        kind="agg_functions",
        output_type="float",
        description="computes the sum of a column",
        documentation="",
        input_fields=[dict(name="column", kind="column", data=["number"])],
    ),
    dict(
        name="rate",
        display_name="rate",
        kind="agg_functions",
        output_type="float",
        description="computes the rate of a 0/1 conditioned on a list of timestamps",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["number"]),
            dict(name="conditioned_on", kind="list_column", data=["timestamp"]),
        ],
    ),
    dict(
        name="average",
        display_name="Average",
        kind="agg_functions",
        output_type="float",
        description="Computes the average of a column",
        documentation="",
        input_fields=[dict(name="column", kind="column", data=["number"])],
    ),
    dict(
        name="max",
        display_name="Max",
        kind="agg_functions",
        output_type="column",
        description="Computes the maximum value of a column",
        documentation="",
        input_fields=[
            dict(
                name="column",
                kind="column",
                data=ALL_TYPES,
            )
        ],
    ),
    dict(
        name="min",
        display_name="Min",
        kind="agg_functions",
        output_type="column",
        description="Computes the minimum value of the column",
        documentation="",
        input_fields=[
            dict(
                name="column",
                kind="column",
                data=ALL_TYPES,
            )
        ],
    ),
    dict(
        name="median",
        display_name="Median",
        kind="agg_functions",
        output_type="float",
        description="Computes the Continuous Median",
        documentation="",
        input_fields=[dict(name="column", kind="column", data=["number"])],
    ),
    dict(
        name="list_agg",
        display_name="List Agg",
        kind="agg_functions",
        output_type="string",
        description="Concatinates all the unique values ordered by time",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["string"]),
            dict(name="base_column", kind="column", data=["timestamp"]),
        ],
    ),
    dict(
        name="list_agg_unique",
        display_name="List Agg Distinct",
        kind="agg_functions",
        output_type="string",
        description="Concatinates all the unique values ordered by time",
        documentation="",
        input_fields=[
            dict(name="column", kind="column", data=["string"]),
            dict(name="base_column", kind="column", data=["timestamp"]),
        ],
    ),
    dict(
        name="percentile_cont",
        display_name="Percentile of",
        kind="agg_functions",
        output_type="float",
        description="Computes the Continuous Percentile (median is .5 percentile but interpolated)",
        documentation="",
        input_fields=[
            dict(name="percentile", kind="float", data=["number"]),
            dict(name="column", kind="column", data=["number"]),
        ],
    ),
    dict(
        name="first_value",
        display_name="First Value",
        kind="agg_functions",
        output_type="string",
        description="Return the first value the column based on the timestamp values of base_column",
        documentation="",
        input_fields=[
            dict(name="base_column", kind="column", data=["timestamp"]),
            dict(name="column", kind="column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="last_value",
        display_name="Last Value",
        kind="agg_functions",
        output_type="string",
        description="Finds the last value in the column based on the timestamp values of the base_column",
        documentation="",
        input_fields=[
            dict(name="base_column", kind="column", data=["timestamp"]),
            dict(name="column", kind="column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="first_value_case",
        display_name="First Value Case",
        kind="agg_functions",
        output_type="string",
        description="Return the first value the column based on the timestamp values of base_column with a case statement",
        documentation="",
        input_fields=[
            dict(name="case", kind="condition", data=["bool"]),
            dict(name="base_column", kind="column", data=["timestamp"]),
            dict(name="column", kind="column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="last_value_case",
        display_name="Last Value Case",
        kind="agg_functions",
        output_type="string",
        description="Finds the last value in the column based on the timestamp values of the base_column with a case statement",
        documentation="",
        input_fields=[
            dict(name="case", kind="condition", data=["bool"]),
            dict(name="base_column", kind="column", data=["timestamp"]),
            dict(name="column", kind="column", data=ALL_TYPES),
        ],
    ),
    dict(
        name="stddev",
        display_name="STD of ",
        kind="agg_functions",
        output_type="float",
        description="Finds the standard deviation of a column",
        documentation="",
        input_fields=[dict(name="column", kind="column", data=["number"])],
    ),
]

FUNCTION_EXAMPLES = [
    dict(
        name="bin",
        description="Bins a column into a set of buckets",
        equation="iff([column] < 10, '0-10', iff([column] < 20, '10-20', '20+'))",
    )
]

WINDOW_FUNCTIONS = [
    f["name"] for f in FUNCTIONS if " over" in (CONFIG["redshift"]["functions"].get(f["name"]) or "").lower()
]


def get_output_type(af, column):
    if af.get("output_type") == "column":
        return column.get("type")
    else:
        return af.get("output_type") or "string"


## mappers to make it easy to swap between functions
def get_old_functions():
    return {f["name"]: new_to_old_function(f) for f in FUNCTIONS}


def new_to_old_function(f):
    f["input_fields"] = [i["name"] for i in f["input_fields"]]
    return f


def generate_doc(kind="function"):
    doc = []

    # go through all the function
    for f in FUNCTIONS:
        if f["kind"] == kind:
            doc.extend(get_pretty_function(f))

    # add some low level notes
    doc.append("VALID Resolutions: {}".format(", ".join(RESOLUTIONS)))
    doc.append("")
    doc.append("VALID TIMEZONES: {}".format(", ".join(TIMEZONES)))

    return "\n".join(doc)


def get_pretty_function(f):
    s = [
        "{}({})".format(f["name"], ", ".join(get_pretty_field(i) for i in f["input_fields"])),
        "\t{}".format(f["description"]),
        "\tReturns: {}".format(f["output_type"]),
        "------------------------------------------------------",
        "",
    ]
    return s


def get_pretty_field(field):
    if field["kind"] == "column":
        return "{{{}}}".format(field["name"].upper())
    elif field["kind"] in ("string", "constraint_list"):
        return "'{}'".format(field["name"])
    elif field["kind"] == "string":
        return "'{}'".format(field["name"])
    elif field["kind"] == "list_column":
        return "[{{{}_COLUMN}},]".format(field["name"].upper())
    else:
        return field["name"]
